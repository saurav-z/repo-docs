# Llama Stack: Build Production-Ready AI Applications with Ease

**Llama Stack simplifies AI application development by providing a unified API layer, flexible deployment options, and a robust ecosystem â€“ [Explore Llama Stack on GitHub](https://github.com/llamastack/llama-stack).**

Llama Stack is your go-to solution for building and deploying generative AI applications. This powerful framework streamlines the development process, offering a consistent and flexible approach to integrating various AI models and services.

## Key Features

*   **Unified API Layer:** Standardizes APIs for Inference, RAG, Agents, Tools, Safety, Evals, and Telemetry.
*   **Plugin Architecture:** Supports diverse API implementations across local, on-premises, cloud, and mobile environments.
*   **Prepackaged Distributions:** Offers ready-to-use solutions for quick and reliable deployments.
*   **Multiple Developer Interfaces:** Provides CLI and SDKs for Python, Typescript, iOS, and Android, simplifying integration.
*   **Standalone Applications:** Offers examples for production-grade AI application development.
*   **Llama 4 Support:** Includes support for Meta's Llama 4 models.

## Benefits of Using Llama Stack

*   **Flexibility:** Choose your preferred infrastructure without changing APIs, supporting various deployment scenarios.
*   **Consistency:** Build, test, and deploy applications with consistent behavior through unified APIs.
*   **Robust Ecosystem:** Integrates with distribution partners (cloud providers, hardware vendors, and AI-focused companies) to offer tailored infrastructure, software, and services.

## Quick Start

Get started with Llama Stack quickly using our one-line installer:

```bash
curl -LsSf https://github.com/meta-llama/llama-stack/raw/main/scripts/install.sh | bash
```

## API Providers

Llama Stack supports a wide array of API providers, enabling you to choose the best tools for your needs.  Check out the full list [here](https://llama-stack.readthedocs.io/en/latest/providers/index.html).

## Distributions

Llama Stack Distributions simplify deployment by pre-configuring provider implementations. Easily transition from local development to production environments without code changes.  See available distributions:

*   Starter Distribution
*   Meta Reference
*   PostgreSQL

## Documentation

Explore detailed documentation to help you get started:

*   [Quick Start Guide](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html)
*   [CLI Reference](https://llama-stack.readthedocs.io/en/latest/references/llama_cli_reference/index.html)
*   [Client SDKs](https://llama-stack.readthedocs.io/en/latest/client_sdks/index.html)
*   [Contributing Guide](CONTRIBUTING.md)

## Client SDKs

Integrate Llama Stack into your preferred programming environment. Client SDKs are available for:

*   **Python:** [llama-stack-client-python](https://github.com/meta-llama/llama-stack-client-python)
*   **Swift:** [llama-stack-client-swift](https://github.com/meta-llama/llama-stack-client-swift)
*   **Typescript:** [llama-stack-client-typescript](https://github.com/meta-llama/llama-stack-client-typescript)
*   **Kotlin:** [llama-stack-client-kotlin](https://github.com/meta-llama/llama-stack-client-kotlin)

##  Contributing

We welcome contributions!  See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=meta-llama/llama-stack&type=Date)](https://www.star-history.com/#meta-llama/llama-stack&Date)

## Contributors

Thanks to all of our amazing contributors!

<a href="https://github.com/meta-llama/llama-stack/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=meta-llama/llama-stack" />
</a>