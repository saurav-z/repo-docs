# MiniGPT-4: Unleashing Visual Language Understanding with Advanced LLMs

**MiniGPT-4, developed by Zhu, Chen, Shen, Li, and Elhoseiny, empowers visual language understanding using state-of-the-art large language models.**  Check out the original repo [here](https://github.com/RiseInRose/MiniGPT-4-ZH) for more details.

**Key Features:**

*   **Visual-Language Alignment:** Leverages a projection layer to align the frozen visual encoder from BLIP-2 with the frozen LLM Vicuna.
*   **Two-Stage Training:** Employs a two-stage training process:
    *   **Stage 1 (Pre-training):**  Pre-training on approximately 5 million image-text pairs, enabling the model to understand images.
    *   **Stage 2 (Fine-tuning):** Fine-tuning with a curated dataset of high-quality image-text pairs generated by the model and ChatGPT, significantly improving generation capabilities and overall usability.
*   **Emergent Capabilities:** Exhibits novel visual language abilities, similar to those demonstrated in GPT-4.
*   **Optimized for Efficiency:**  The fine-tuning phase is computationally efficient, requiring only a single A100 for about 7 minutes.
*   **Demo:** Offers an online demo to interact with and understand images.

## Quickstart

### 1. Installation

**Prerequisites:**

*   Clone the repository:
    ```bash
    git clone https://github.com/Vision-CAIR/MiniGPT-4.git
    cd MiniGPT-4
    ```
*   Create and activate a Python environment:
    ```bash
    conda env create -f environment.yml
    conda activate minigpt4
    ```

### 2. Prepare Vicuna Weights

*   **Download Pre-trained Weights:** (Potentially will change due to license restriction) Refer to the original README for download link.
    *   **Alternative:** Prepare Vicuna Weights manually. Follow the instructions below:

    *   **Download Vicuna-13B delta weights:**
        ```bash
        git lfs install
        git clone https://huggingface.co/lmsys/vicuna-13b-delta-v1.1
        ```
    *   **Download LLaMA-13B weights:** (See Original README for guidance)
        *   You can obtain LLaMA weights via [this link](https://github.com/facebookresearch/llama/issues/149) or other provided resources.
    *   **Convert Weights:**
        *   Install dependencies:
            ```bash
            pip install transformers[sentencepiece]
            ```
        *   Convert the weights using the `convert_llama_weights_to_hf.py` script (available at the original repo):
            ```bash
            python src/transformers/models/llama/convert_llama_weights_to_hf.py --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path
            ```
            *   Ensure you replace `/path/to/downloaded/llama/weights` and `/output/path` with the appropriate paths.
    *   **Apply Delta Weights:**
        ```bash
        pip install git+https://github.com/lm-sys/FastChat.git@v0.1.10
        python -m fastchat.model.apply_delta --base /path/to/llama-13b-hf/  --target /path/to/save/working/vicuna/weight/  --delta /path/to/vicuna-13b-delta-v0/
        ```
    *   **Weight Structure:** The final weights folder should have the structure described in the original README.
    *   **Configuration:** Set the path to your Vicuna weights in the model configuration file (see the original README).

### 3. Prepare MiniGPT-4 Checkpoints

*   **Download Pre-trained Checkpoint:** Download the pre-trained checkpoint aligned with your Vicuna model (13B or 7B) from the links provided in the original README.
*   **Configuration:**  Set the checkpoint path in the evaluation configuration file (see the original README).

### 4. Run the Demo Locally

```bash
python demo.py --cfg-path eval_configs/minigpt4_eval.yaml  --gpu-id 0
```

*   The demo.py file launches a local interactive demo.
*   Vicuna defaults to 8-bit loading to save GPU memory.
*   You can modify the `low_resource` setting in `minigpt4_eval.yaml` for higher GPU memory usage and 16-bit operation.

## Training

MiniGPT-4 training includes two alignment stages.

**1. Stage 1: Pre-training**

*   **Dataset:** Use image-text pairs from Laion and CC datasets. See the original README for instructions.
*   **Command:**
    ```bash
    torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigpt4_stage1_pretrain.yaml
    ```
*   **Checkpoint:** First stage checkpoints are available for download.

**2. Stage 2: Fine-tuning**

*   **Dataset:**  Use the curated image-text dataset as described in the original README.
*   **Configuration:** Specify the Stage 1 checkpoint path and output path in `train_configs/minigpt4_stage2_finetune.yaml`.
*   **Command:**
    ```bash
    torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigpt4_stage2_finetune.yaml
    ```

## Acknowledgements

*   [BLIP2](https://huggingface.co/docs/transformers/main/model_doc/blip-2)
*   [Lavis](https://github.com/salesforce/LAVIS)
*   [Vicuna](https://github.com/lm-sys/FastChat)

## Citation

```bibtex
@misc{zhu2022minigpt4,
      title={MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models},
      author={Deyao Zhu and Jun Chen and Xiaoqian Shen and xiang Li and Mohamed Elhoseiny},
      year={2023},
}
```

## License

This repository is licensed under the [BSD 3-Clause License](LICENSE.md).  Code is based on [Lavis](https://github.com/salesforce/LAVIS), under the BSD 3-Clause License [here](LICENSE_Lavis.md).