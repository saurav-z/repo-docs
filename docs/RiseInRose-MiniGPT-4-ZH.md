# MiniGPT-4: Unleash the Power of Visual Language Understanding with Advanced LLMs

**MiniGPT-4, developed by Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny, revolutionizes visual language understanding by leveraging cutting-edge large language models.** [Original Repo](https://github.com/RiseInRose/MiniGPT-4-ZH)

## Key Features

*   **Visual Language Alignment:** Seamlessly integrates a frozen visual encoder (BLIP-2) with the powerful Vicuna language model.
*   **Two-Stage Training:** Employs a two-stage training process, including pretraining and fine-tuning with a high-quality dataset generated by the model itself.
*   **Enhanced Generation Capabilities:** Achieves impressive results in visual language understanding, offering capabilities akin to those of GPT-4.
*   **Open-Source and Accessible:** Leveraging Vicuna, MiniGPT-4 provides an open-source solution for those seeking to build, deploy and contribute to visual-language models.
*   **Online Demo:** Interact with a live demo to explore MiniGPT-4's capabilities.

## Get Started

### Online Demo

Experience MiniGPT-4 firsthand! Click the image below to chat with the model and learn about your images:

[![demo](figs/online_demo.png)](https://minigpt-4.github.io)

Explore more examples on the [Project Page](https://minigpt-4.github.io).

<a href='https://minigpt-4.github.io'><img src='https://img.shields.io/badge/Project-Page-Green'></a>  <a href='MiniGPT_4.pdf'><img src='https://img.shields.io/badge/Paper-PDF-red'></a> <a href='https://huggingface.co/spaces/Vision-CAIR/minigpt4'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'></a> <a href='https://huggingface.co/Vision-CAIR/MiniGPT-4'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue'></a> [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1OK4kYsZphwt5DXchKkzMBjYF6jnkqh4R?usp=sharing) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://www.youtube.com/watch?v=__tftoxpBAw&feature=youtu.be)

### Installation

**1. Prepare Code and Environment:**

```bash
git clone https://github.com/Vision-CAIR/MiniGPT-4.git
cd MiniGPT-4
conda env create -f environment.yml
conda activate minigpt4
```

**2. Prepare the Pretrained Vicuna Weights**
 Instructions and links to download the Vicuna weights and prepare them for use are found in the original repository's README.

**3. Prepare the Pretrained MiniGPT-4 Checkpoint:**

*   Download the pretrained checkpoint aligned with your chosen Vicuna model (13B or 7B). Links are provided in the original README.
*   Set the path to the pretrained checkpoint in `eval_configs/minigpt4_eval.yaml` (line 11).

### Local Demo

Run the demo locally with:

```bash
python demo.py --cfg-path eval_configs/minigpt4_eval.yaml  --gpu-id 0
```

**Note:** For optimizing GPU memory usage, refer to the original README for configurations and model trimming options.

### Training

MiniGPT-4 training involves two alignment stages.  Detailed instructions on dataset preparation, training scripts and configurations are available in the original repository, including:
*   **Stage 1 Pretraining:** Prepare the dataset per the instructions in `dataset/README_1_STAGE.md`. Then, run the training command:

```bash
torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigpt4_stage1_pretrain.yaml
```
*   **Stage 2 Finetuning:**  Prepare the dataset per the instructions in `dataset/README_2_STAGE.md`. Then, specify the path to your stage 1 checkpoint in `train_configs/minigpt4_stage2_finetune.yaml` and run the training command:

```bash
torchrun --nproc-per-node NUM_GPU train.py --cfg-path train_configs/minigpt4_stage2_finetune.yaml
```

## Acknowledgements

*   **BLIP-2:** MiniGPT-4's architecture is based on BLIP-2.
*   **LAVIS:** This repository is built upon LAVIS.
*   **Vicuna:** Thanks to the amazing language capabilities of Vicuna.

## Citation

```bibtex
@misc{zhu2022minigpt4,
      title={MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models}, 
      author={Deyao Zhu and Jun Chen and Xiaoqian Shen and xiang Li and Mohamed Elhoseiny},
      year={2023},
}
```

## License

This project is licensed under the [BSD 3-Clause License](LICENSE.md).
Many of the code is based on [Lavis](https://github.com/salesforce/LAVIS) and is licensed under the BSD 3-Clause License [here](LICENSE_Lavis.md).

## Resources

*   [Original Repository](https://github.com/RiseInRose/MiniGPT-4-ZH)