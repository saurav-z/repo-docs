# Context Engineering: Unlock the Full Potential of LLMs

**Go beyond simple prompts!** This repository provides a comprehensive guide to **context engineering**, the art and science of designing, orchestrating, and optimizing the information a Large Language Model (LLM) sees beyond your initial prompt, offering the power to unlock next-level performance. Explore the original repo here:  [Context-Engineering](https://github.com/davidkimai/Context-Engineering)

**Key Features:**

*   **Comprehensive Course:** A first-principles handbook under development, covering everything from basic concepts to advanced techniques, inspired by cutting-edge research.
*   **AgenticOS:** Support for various LLM tools including Claude Code, OpenCode, Amp, Kiro, Codex, and Gemini CLI.
*   **Hands-on Tutorials & Examples:** Practical tutorials and example projects to get you started with context engineering.
*   **First Principles and Progressive Learning:**  A structured approach for mastering LLMs.
*   **Cutting-Edge Research Integration:** Incorporates the latest findings from leading AI research institutions.

## What is Context Engineering?

Context Engineering moves beyond prompt engineering to encompass the entire context window, including examples, memory, retrieval, tools, state, and control flow.  This approach equips LLMs with "cognitive tools" to increase performance.

```
                    Prompt Engineering  │  Context Engineering
                       ↓                │            ↓                      
               "What you say"           │  "Everything else the model sees"
             (Single instruction)       │    (Examples, memory, retrieval,
                                        │     tools, state, control flow)
```

## Core Concepts & Benefits

*   **Improve LLM Performance:** The key to unlocking more capabilities in LLMs.
*   **Token Budget Optimization:**  Efficiency, cost, and speed gains.
*   **Few-Shot Learning:** Teach models using in-context learning.
*   **Memory Systems:** Enable stateful, coherent interactions.
*   **Retrieval Augmentation:** Ensure responses are grounded in facts, reducing hallucination.
*   **Cognitive Tools & Prompt Programming:** Build custom tools and templates.
*   **Neural Field Theory:** Model context as a dynamic neural field for iterative context updates.
*   **Symbolic Mechanisms:** Enable higher order reasoning with symbolic architectures.

## Learning Path

A structured journey designed for both beginners and experienced users.

1.  **Foundations:** Discover fundamental concepts.
2.  **Hands-on:** Experiment with tutorials and examples.
3.  **Deep Dives:** Explore reference materials.
4.  **Community Contributions:** Share knowledge and collaborate.

## Ready to Dive In?

1.  **Get Started**:  Begin with the `00_foundations/01_atoms_prompting.md` (5 min read).
2.  **Experiment:** Try the minimal example, `10_guides_zero_to_one/01_min_prompt.ipynb`.
3.  **Explore:** Adapt the provided templates.
4.  **Implement:**  Examine the example projects and their associated code.

## Research Highlights

This repository leverages research to enhance LLM capabilities:

*   **Memory & Reasoning:**  MEM1—learning to synergize memory and reasoning for efficient long-horizon agents.
*   **Cognitive Tools:**  Eliciting Reasoning in Language Models with Cognitive Tools using "Cognitive Tools" to unlock a modular approach.
*   **Emergent Symbols:**  Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models.

## Contributions

Help build the future of context engineering! See [CONTRIBUTING.md](.github/CONTRIBUTING.md) for contribution guidelines.

## Citation

```bibtex
@misc{context-engineering,
  author = {Context Engineering Contributors},
  title = {Context Engineering: Beyond Prompt Engineering},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/davidkimai/context-engineering}
}