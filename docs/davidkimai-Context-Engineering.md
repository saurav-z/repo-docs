# Context Engineering: Mastering the Art & Science of LLM Context

**Unlock the true potential of Large Language Models (LLMs) by mastering context engineering—the key to crafting intelligent systems that go beyond simple prompts.** ([Original Repository](https://github.com/davidkimai/Context-Engineering))

This repository provides a comprehensive, first-principles approach to context engineering, moving beyond prompt engineering to the wider discipline of context design, orchestration, and optimization.  Inspired by the work of Andrej Karpathy and the latest research, this handbook guides you from foundational concepts to cutting-edge techniques.

## Key Features

*   **Comprehensive Learning Path:** Explore a structured curriculum, from fundamental principles to advanced applications, with hands-on tutorials and practical examples.
*   **First-Principles Approach:** Understand the core concepts and build a strong foundation in context design.
*   **Visual Learning:** Benefit from clear explanations, diagrams, and code examples for effective learning.
*   **Practical Implementation:** Access reusable templates, code snippets, and real-world examples to accelerate your projects.
*   **Cutting-Edge Research:**  Stay ahead of the curve with insights from the latest research papers and breakthroughs.

## Core Concepts

This repository focuses on the concept of "context" as defined in research, which is more than just the prompt. It covers the following:

*   **Token Budget Optimization:** Learn to manage and optimize the number of tokens in your context.
*   **Few-Shot Learning:** Utilize examples and demonstration sets to teach LLMs.
*   **Memory Systems:** Persist information across turns for stateful interactions.
*   **Retrieval Augmentation (RAG):** Retrieve and inject relevant documents to ground responses in facts.
*   **Control Flow:** Break down complex tasks into manageable steps.
*   **Context Pruning:** Remove irrelevant information to improve performance.
*   **Metrics & Evaluation:** Measure and iteratively improve the effectiveness of your context design.
*   **Cognitive Tools & Prompt Programming:** Build custom tools and templates to create context engineering layers.
*   **Neural Field Theory:** Modeling context as a dynamic neural field for iterative updating.
*   **Symbolic Mechanisms:** Implement symbolic architectures to allow for higher-order reasoning.
*   **Quantum Semantics:** Design context systems leveraging superpositional techniques.

## Learning Path & Resources

*   **[Comprehensive Course Under Construction](https://github.com/davidkimai/Context-Engineering/tree/main/00_COURSE)**: Dive deep into the course content as it is developed.
*   **Foundations:**  (e.g., `00_foundations/01_atoms_prompting.md`) Understand prompts and how they work, building from the ground up.
*   **Guides & Tutorials:** (e.g., `10_guides_zero_to_one/01_min_prompt.py`) Hands-on walkthroughs with real code to get you started.
*   **Templates:** (e.g., `20_templates/minimal_context.yaml`) Copy and paste code snippets for quick implementation.
*   **Examples:**  (e.g., `30_examples/00_toy_chatbot/`) Explore practical real-world projects.
*   **Reference:** Deep dives and evaluation checklists for advanced learning.
*   **Contribution:** Find community projects to engage in.
*   **Protocols:** Protocol shells and frameworks for advanced usage.
*   **Agents:** Demonstration of agent usage.
*   **Field Integration:** Field-based project examples.
*   **Cognitive Tools:** Cognitive framework for reasoning.

## Inspired By...

> *"Context engineering is the delicate art and science of filling the context window with just the right information for the next step." — [Andrej Karpathy](https://x.com/karpathy/status/1937902205765607626)*

Andrej Karpathy's insights form the foundation of this work. The learning style is directly inspired by:

*   **First principles** - Start with the fundamental context.
*   **Iterative add-on** - Add only what the model demonstrably lacks.
*   **Measure everything** - Measure token cost, latency, and quality score.
*   **Delete ruthlessly** - Pruning beats padding.
*   **Code > slides** - Every concept has a runnable cell.
*   **Visualize everything** - Every concept is visualized with ASCII and symbolic diagrams.

## Research Highlights

This repository leverages and builds upon the latest research in the field of context engineering. Here are a few key examples:

*   **[MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents - Singapore-MIT June 2025](https://www.arxiv.org/pdf/2506.15841)**  Memory consolidation for long-horizon agents, compressing interactions into a compact “internal state”.
*   **[Eliciting Reasoning in Language Models with Cognitive Tools - IBM Zurich June 2025](https://www.arxiv.org/pdf/2506.12115)** Implementing cognitive tools, using prompt templates for reasoning operations.
*   **[Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models - ICML Princeton June 18, 2025](https://openreview.net/forum?id=y1SnRPDWx4)** Development of inner symbolic "logic circuits," allowing LLMs to reason.

## Links

*   **Original Repository:** [https://github.com/davidkimai/Context-Engineering](https://github.com/davidkimai/Context-Engineering)
*   **[DeepWiki](https://deepwiki.com/davidkimai/Context-Engineering)**
*   **[DeepGraph](https://www.deepgraph.co/davidkimai/Context-Engineering)**
*   **[Chat with NotebookLM + Podcast Deep Dive](https://notebooklm.google.com/notebook/0c6e4dc6-9c30-4f53-8e1a-05cc9ff3bc7e)**
*   **[Discord](https://discord.gg/JeFENHNNNQ)**

## Contributing

We welcome your contributions! Please see the [CONTRIBUTING.md](.github/CONTRIBUTING.md) file for guidelines.

## License

This project is licensed under the [MIT License](LICENSE).

## Citation

```bibtex
@misc{context-engineering,
  author = {Context Engineering Contributors},
  title = {Context Engineering: Beyond Prompt Engineering},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/davidkimai/context-engineering}
}
```