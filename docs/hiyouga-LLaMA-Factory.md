<!-- ALL-IN-ONE SEO-OPTIMIZED README -->
<div align="center">
  <img src="assets/logo.png" alt="LLaMA Factory Logo" width="400">
</div>

<!-- SOCIAL LINKS & BADGES -->
[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)
[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)
[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)
[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)
[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)
[![Citation](https://img.shields.io/badge/citation-651-green)](https://scholar.google.com/scholar?cites=12620864006390196564)
[![Docker Pulls](https://img.shields.io/docker/pulls/hiyouga/llamafactory)](https://hub.docker.com/r/hiyouga/llamafactory/tags)
[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)
[![Discord](https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&style=flat)](https://discord.gg/rKfvV9r9FK)
[![GitCode](https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg)](https://gitcode.com/zhengyaowei/LLaMA-Factory)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)
[![Open in DSW](https://gallery.pai-ml.com/assets/open-in-dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)
[![Open in Alaya](assets/alaya_new.svg)](https://docs.alayanew.com/docs/documents/newActivities/llamafactory/?utm_source=LLaMA-Factory)
[![Open in Spaces](https://img.shields.io/badge/ü§ó-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)
[![Open in Studios](https://img.shields.com/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)
[![Open in Novita](https://img.shields.io/badge/Novita-Deploy%20Template-blue)](https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47)

<!-- SUPPORTED BY SECTION -->
### Used by [Amazon](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/), [NVIDIA](https://developer.nvidia.com/rtx/ai-toolkit), [Aliyun](https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory), etc.

<!-- SPONSORSHIP -->
<div align="center" markdown="1">
### Supporters ‚ù§Ô∏è
<a href="https://warp.dev/llama-factory">
    <img alt="Warp sponsorship" width="400" src="https://github.com/user-attachments/assets/ab8dd143-b0fd-4904-bdc5-dd7ecac94eae">
</a>
#### [Warp, the agentic terminal for developers](https://warp.dev/llama-factory)
[Available for MacOS, Linux, & Windows](https://warp.dev/llama-factory)

<hr>
<!-- SUMMARY & HOOK -->
<!-- Summary with Keywords -->
**LLaMA Factory empowers you to fine-tune and deploy over 100 large language models with ease, offering both code-free and advanced options!**

<!-- Quick Start -->
[Easily fine-tune 100+ large language models with zero-code [CLI](#quickstart) and [Web UI](#fine-tuning-with-llama-board-gui-powered-by-gradio)]

<!-- Join Our Community -->
üëã Join our [WeChat group](assets/wechat.jpg), [NPU user group](assets/wechat_npu.jpg) or [Alaya NeW user group](assets/wechat_alaya.png).

<!-- LANGUAGE SWITCHER -->
\[ [English](README.md) | [‰∏≠Êñá](README_zh.md) \]

<!-- DEMO -->
<!-- Fine-tuning GIF -->
**Fine-tuning a large language model can be easy as...**
<div align="center">
  <img src="https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e" alt="Fine-tuning Demo" width="600">
</div>

<!-- ACCESS POINTS -->
Choose your path:

-   **Documentation (WIP)**: [https://llamafactory.readthedocs.io/en/latest/](https://llamafactory.readthedocs.io/en/latest/)
-   **Documentation (AMD GPU)**: [https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html](https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html)
-   **Colab (free)**: [https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)
-   **Local machine**: Please refer to [usage](#getting-started)
-   **PAI-DSW (free trial)**: [https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)
-   **Alaya NeW (cloud GPU deal)**: [https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory](https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory)

> [!NOTE]
> Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.
</div>

<!-- TABLE OF CONTENTS -->
## Table of Contents

-   [Key Features](#key-features)
-   [Supported Models](#supported-models)
-   [Supported Training Approaches](#supported-training-approaches)
-   [Getting Started](#getting-started)
    -   [Installation](#installation)
    -   [Quickstart](#quickstart)
    -   [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)
    -   [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)
-   [Changelog](#changelog)
-   [Provided Datasets](#provided-datasets)
-   [Projects using LLaMA Factory](#projects-using-llama-factory)
-   [License](#license)
-   [Citation](#citation)
-   [Acknowledgement](#acknowledgement)

<!-- KEY FEATURES -->
## Key Features

*   **Wide Model Support**: Fine-tune LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, DeepSeek, Yi, Gemma, and more.
*   **Flexible Training Methods**: Utilize pre-training, SFT, reward modeling, PPO, DPO, and other advanced training approaches.
*   **Efficient Resource Utilization**: Achieve 16-bit full-tuning, freeze-tuning, LoRA, and low-bit quantization (QLoRA) with various methods.
*   **Advanced Algorithms**: Integrate cutting-edge algorithms like GaLore, BAdam, APOLLO, Adam-mini, Muon, DoRA, LongLoRA, LoftQ, and PiSSA.
*   **Performance Enhancements**: Leverage FlashAttention-2, Unsloth, Liger Kernel, and other practical optimizations.
*   **Task Versatility**: Handle multi-turn dialogue, tool use, image understanding, and other diverse tasks.
*   **Experiment Tracking**: Monitor experiments using LlamaBoard, TensorBoard, Wandb, MLflow, and SwanLab.
*   **Faster Inference**: Deploy with OpenAI-style API, Gradio UI, and CLI using vLLM or SGLang.

<!-- SUPPORTED MODELS SECTION -->
## Supported Models

*   **Baichuan 2**
*   **BLOOM/BLOOMZ**
*   **ChatGLM3**
*   **Command R**
*   **DeepSeek (Code/MoE)**
*   **Falcon**
*   **Gemma/Gemma 2/CodeGemma**
*   **GLM-4/GLM-4.1V/GLM-Z1**
*   **GPT-2**
*   **Granite 3.0-3.3**
*   **Hunyuan**
*   **InternLM 2-3/InternVL 2.5-3**
*   **Llama/Llama 2/Llama 3/Llama 4/Llama 3.2 Vision**
*   **LLaVA-1.5**
*   **MiMo**
*   **MiniCPM**
*   **Mistral/Mixtral**
*   **Qwen (1-2.5)**
*   **Seed Coder**
*   **StarCoder 2**
*   **Yi/Yi-VL**
*   **Yuan 2**

<!-- SUPPORTED TRAINING APPROACHES SECTION -->
## Supported Training Approaches

*   Pre-Training
*   Supervised Fine-Tuning
*   Reward Modeling
*   PPO Training
*   DPO Training
*   KTO Training
*   ORPO Training
*   SimPO Training

<!-- GETTING STARTED -->
## Getting Started

### Installation

1.  **Install from Source:** Follow the instructions for cloning the repository and installing dependencies using `pip`.
2.  **Docker Installation**: Use the Docker commands to pull and run a pre-built image or build your own.
3.  **Virtual Environment with uv**: Follow the uv installation guide
4.  **Windows Installation Notes**: Steps for installing PyTorch, bitsandbytes, and Flash Attention-2 on Windows.
5.  **Ascend NPU Installation**: Instructions for installing Ascend CANN Toolkit, CANN Kernels, and other specific requirements for Ascend NPU devices.

### Quickstart

1.  **Fine-tuning:** Use the `llamafactory-cli train` command with a configuration file.
2.  **Inference:** Use the `llamafactory-cli chat` command for inference.
3.  **Merging:** Use the `llamafactory-cli export` command to merge LoRA weights.

### Fine-Tuning with LLaMA Board GUI

Run `llamafactory-cli webui` to use the GUI for training, evaluation, and inference.

### Deploy with OpenAI-style API and vLLM

Use the `llamafactory-cli api` command to deploy with an OpenAI-style API and vLLM backend.

<!-- CHANGELOG SECTION -->
## Changelog

Stay up-to-date with the latest features and improvements.  See [Changelog](#changelog)

<!-- PROVIDED DATASETS -->
## Provided Datasets

*   **Pre-training Datasets:** Wiki Demo, RefinedWeb, RedPajama V2, Wikipedia, etc.
*   **Supervised Fine-tuning Datasets:** Identity, Stanford Alpaca, Alpaca GPT4, BELLE, UltraChat, etc.
*   **Preference Datasets:** DPO mixed, UltraFeedback, COIG-P, etc.

<!-- PROJECTS USING LLAMA FACTORY SECTION -->
## Projects using LLaMA Factory

Explore a list of projects that utilize LLaMA Factory.  See [Projects using LLaMA Factory](#projects-using-llama-factory)

<!-- LICENSE SECTION -->
## License

This project is licensed under the [Apache-2.0 License](LICENSE).

<!-- CITATION SECTION -->
## Citation

If you find this project helpful, please cite the following paper: [Citation](#citation)

<!-- ACKNOWLEDGEMENT SECTION -->
## Acknowledgement

This project benefits from PEFT, TRL, QLoRA and FastChat. Thanks for their great work!