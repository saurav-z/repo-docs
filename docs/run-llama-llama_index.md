# ðŸ—‚ï¸ LlamaIndex: The Data Framework for LLM Applications

> Supercharge your Large Language Model (LLM) applications by connecting them to your data with LlamaIndex, a powerful data framework.

[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-index)](https://pypi.org/project/llama-index/)
[![Build](https://github.com/run-llama/llama_index/actions/workflows/build_package.yml/badge.svg)](https://github.com/run-llama/llama_index/actions/workflows/build_package.yml)
[![GitHub contributors](https://img.shields.io/github/contributors/jerryjliu/llama_index)](https://github.com/jerryjliu/llama_index/graphs/contributors)
[![Discord](https://img.shields.io/discord/1059199217496772688)](https://discord.gg/dGcwcsnxhU)
[![Twitter](https://img.shields.io/twitter/follow/llama_index)](https://x.com/llama_index)
[![Reddit](https://img.shields.io/reddit/subreddit-subscribers/LlamaIndex?style=plastic&logo=reddit&label=r%2FLlamaIndex&labelColor=white)](https://www.reddit.com/r/LlamaIndex/)
[![Ask AI](https://img.shields.io/badge/Phorm-Ask_AI-%23F2777A.svg?&logo=data:image/svg+xml;base64,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)](https://www.phorm.ai/query?projectId=c5863b56-6703-4a5d-87b6-7e6031bf16b6)

**[View the source code on GitHub](https://github.com/run-llama/llama_index)**

LlamaIndex (formerly GPT Index) provides a powerful and flexible framework for building applications that leverage Large Language Models (LLMs) with your own data.

### Key Features:

*   **Data Connectors:** Ingest data from various sources and formats (APIs, PDFs, documents, SQL, etc.).
*   **Data Structuring:** Structure your data (indices, graphs) for optimal use with LLMs.
*   **Advanced Retrieval/Query Interface:**  Retrieve relevant context and generate knowledge-augmented output based on LLM input prompts.
*   **Seamless Integrations:** Integrate with popular frameworks like LangChain, Flask, and others.
*   **LlamaHub:** Access a community library of data loaders for easy integration.
*   **Comprehensive Documentation:** Access detailed documentation with tutorials, guides, and references.

### Getting Started

LlamaIndex offers two primary installation methods:

1.  **Starter Package:**  `llama-index`: Includes core functionality and a selection of integrations.

2.  **Customized Installation:**  `llama-index-core`: Install the core package and then add specific integration packages from [LlamaHub](https://llamahub.ai/) based on your needs.  This allows you to choose your preferred LLM, embedding, and vector store providers.

   Use `llama-index-core` if you need a high degree of customization or want to minimize your dependencies.

### Important Links

*   **LlamaIndex.TS (Typescript/Javascript):**  [https://github.com/run-llama/LlamaIndexTS](https://github.com/run-llama/LlamaIndexTS)
*   **Documentation:** [https://docs.llamaindex.ai/en/stable/](https://docs.llamaindex.ai/en/stable/)
*   **X (formerly Twitter):**  [https://x.com/llama_index](https://x.com/llama_index)
*   **LinkedIn:** [https://www.linkedin.com/company/llamaindex/](https://www.linkedin.com/company/llamaindex/)
*   **Reddit:** [https://www.reddit.com/r/LlamaIndex/](https://www.reddit.com/r/LlamaIndex/)
*   **Discord:** [https://discord.gg/dGcwcsnxhU](https://discord.gg/dGcwcsnxhU)

### Ecosystem

*   **LlamaHub:** [https://llamahub.ai](https://llamahub.ai)
*   **LlamaLab:** [https://github.com/run-llama/llama-lab](https://github.com/run-llama/llama-lab)

## ðŸš€ Overview

**Note:**  For the most up-to-date information, please refer to the [documentation](https://docs.llamaindex.ai/en/stable/).

### Context

Large Language Models (LLMs) excel at knowledge generation and reasoning, but they are pre-trained on public data.  To unlock the full potential of LLMs, we need to augment them with our private data.

### Proposed Solution

LlamaIndex addresses this need by providing a comprehensive "data framework" to facilitate building LLM applications. It offers:

*   **Data Ingestion:**  Connectors for various data sources and formats.
*   **Data Structuring:**  Tools for structuring your data into indices and graphs, optimized for LLM use.
*   **Intelligent Retrieval:** An advanced query interface to retrieve context and generate knowledge-augmented outputs based on your data.
*   **Application Integration:** Easy integration with various application frameworks (e.g., LangChain, Flask).

LlamaIndex offers both a high-level API for ease of use and lower-level APIs for customization and extension. This empowers both beginners and advanced users to build powerful LLM applications.

## ðŸ’¡ Contributing

We welcome contributions!  Contribute to LlamaIndex core or build integrations that extend its functionality. See our [Contribution Guide](CONTRIBUTING.md).

## ðŸ’» Example Usage

**(Ensure you have the necessary dependencies installed, e.g., `pip install llama-index-core llama-index-llms-openai`)**

```python
import os

os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY"

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# Load data from a directory
documents = SimpleDirectoryReader("YOUR_DATA_DIRECTORY").load_data()

# Build a vector index
index = VectorStoreIndex.from_documents(documents)

# Create a query engine
query_engine = index.as_query_engine()

# Query the index
response = query_engine.query("YOUR_QUESTION")
print(response)
```

**Storing and Loading Data:**

```python
# Persist the index to disk
index.storage_context.persist()

# Load the index from disk
from llama_index.core import StorageContext, load_index_from_storage

storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)
```

## ðŸ“– Citation

```
@software{Liu_LlamaIndex_2022,
author = {Liu, Jerry},
doi = {10.5281/zenodo.1234},
month = {11},
title = {{LlamaIndex}},
url = {https://github.com/jerryjliu/llama_index},
year = {2022}
}