[![Unit-tests](https://github.com/pytorch/rl/actions/workflows/test-linux.yml/badge.svg)](https://github.com/pytorch/rl/actions/workflows/test-linux.yml)
[![Documentation](https://img.shields.io/badge/Documentation-blue.svg)](https://pytorch.org/rl/)
[![Benchmarks](https://img.shields.io/badge/Benchmarks-blue.svg)](https://pytorch.github.io/rl/dev/bench/)
[![codecov](https://codecov.io/gh/pytorch/rl/branch/main/graph/badge.svg?token=HcpK1ILV6r)](https://codecov.io/gh/pytorch/rl)
[![Twitter Follow](https://img.shields.io/twitter/follow/torchrl1?style=social)](https://twitter.com/torchrl1)
[![Python version](https://img.shields.io/pypi/pyversions/torchrl.svg)](https://www.python.org/downloads/)
[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/pytorch/rl/blob/main/LICENSE)
<a href="https://pypi.org/project/torchrl"><img src="https://img.shields.io/pypi/v/torchrl" alt="pypi version"></a>
<a href="https://pypi.org/project/torchrl-nightly"><img src="https://img.shields.io/pypi/v/torchrl-nightly?label=nightly" alt="pypi nightly version"></a>
[![Downloads](https://static.pepy.tech/personalized-badge/torchrl?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads)](https://pepy.tech/project/torchrl)
[![Downloads](https://static.pepy.tech/personalized-badge/torchrl-nightly?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads%20(nightly))](https://pepy.tech/project/torchrl-nightly)
[![Discord Shield](https://dcbadge.vercel.app/api/server/cZs26Qq3Dd)](https://discord.gg/cZs26Qq3Dd)

# TorchRL: Your Gateway to Advanced Reinforcement Learning with PyTorch

TorchRL provides a flexible, efficient, and modular framework for reinforcement learning research and development in PyTorch. Learn more at the [original repository](https://github.com/pytorch/rl).

## Key Features

*   **Python-First:** Designed with Python for ease of use and flexibility.
*   **Efficient:** Optimized for high-performance RL research.
*   **Modular & Customizable:** Easily swap, transform, or create new components.
*   **Well-Documented:** Comprehensive documentation and tutorials.
*   **Rigorously Tested:** Reliability and stability are ensured.
*   **Reusable Functionals:** Offers a set of highly reusable functions for cost functions, returns, and data processing.
*   **LLM API:** Complete framework for language model fine-tuning.

    *   Unified LLM Wrappers (Hugging Face and vLLM).
    *   Conversation Management with automatic chat template detection.
    *   Tool Integration (Python code execution, function calling).
    *   Specialized Objectives (GRPO, SFT).
    *   High-Performance Async Collectors.

## What's New

### LLM API - Complete Framework for Language Model Fine-tuning

TorchRL now includes a comprehensive **LLM API** for post-training and fine-tuning of language models! This new framework provides everything you need for RLHF, supervised fine-tuning, and tool-augmented training:

- ü§ñ **Unified LLM Wrappers**: Seamless integration with Hugging Face models and vLLM inference engines - more to come!
- üí¨ **Conversation Management**: Advanced [`History`](torchrl/data/llm/history.py) class for multi-turn dialogue with automatic chat template detection
- üõ†Ô∏è **Tool Integration**: [Built-in support](torchrl/envs/llm/transforms/) for Python code execution, function calling, and custom tool transforms
- üéØ **Specialized Objectives**: [GRPO](torchrl/objectives/llm/grpo.py) (Group Relative Policy Optimization) and [SFT](torchrl/objectives/llm/sft.py) loss functions optimized for language models
- ‚ö° **High-Performance Collectors**: [Async data collection](torchrl/collectors/llm/) with distributed training support
- üîÑ **Flexible Environments**: Transform-based architecture for reward computation, data loading, and conversation augmentation

The LLM API follows TorchRL's modular design principles, allowing you to mix and match components for your specific use case. Check out the [complete documentation](https://pytorch.org/rl/main/reference/llms.html) and [GRPO implementation example](https://github.com/pytorch/rl/tree/main/sota-implementations/grpo) to get started!

<details>
  <summary>Quick LLM API Example</summary>

```python
from torchrl.envs.llm import ChatEnv
from torchrl.modules.llm import TransformersWrapper
from torchrl.objectives.llm import GRPOLoss
from torchrl.collectors.llm import LLMCollector

# Create environment with Python tool execution
env = ChatEnv(
    tokenizer=tokenizer,
    system_prompt="You are an assistant that can execute Python code.",
    batch_size=[1]
).append_transform(PythonInterpreter())

# Wrap your language model
llm = TransformersWrapper(
    model=model,
    tokenizer=tokenizer,
    input_mode="history"
)

# Set up GRPO training
loss_fn = GRPOLoss(llm, critic, gamma=0.99)
collector = LLMCollector(env, llm, frames_per_batch=100)

# Training loop
for data in collector:
    loss = loss_fn(data)
    loss.backward()
    optimizer.step()
```

</details>

## Getting Started

Jumpstart your RL journey with our [Getting Started tutorials](https://pytorch.org/rl/stable/index.html#getting-started).

## Documentation and Knowledge Base

*   **Documentation:** Access comprehensive documentation [here](https://pytorch.org/rl).
*   **RL Knowledge Base:**  Find helpful resources and tutorials [here](https://pytorch.org/rl/stable/reference/knowledge_base.html).

## Spotlight Publications

TorchRL is a versatile tool used across many domains. Here are a few examples:

*   [ACEGEN](https://pubs.acs.org/doi/10.1021/acs.jcim.4c00895): Reinforcement Learning of Generative Chemical Agents
    for Drug Discovery
*   [BenchMARL](https://www.jmlr.org/papers/v25/23-1612.html): Benchmarking Multi-Agent Reinforcement Learning
*   [BricksRL](https://arxiv.org/abs/2406.17490): A Platform for Democratizing Robotics and Reinforcement Learning
    Research and Education with LEGO
*   [OmniDrones](https://ieeexplore.ieee.org/abstract/document/10409589): An Efficient and Flexible Platform for Reinforcement Learning in Drone Control
*   [RL4CO](https://arxiv.org/abs/2306.17100): an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark
*   [Robohive](https://proceedings.neurips.cc/paper_files/paper/2023/file/8a84a4341c375b8441b36836bb343d4e-Paper-Datasets_and_Benchmarks.pdf): A unified framework for robot learning

## Writing Simplified and Portable RL Codebase with `TensorDict`

TorchRL leverages `TensorDict` ([documentation](https://pytorch.github.io/tensordict/)) for streamlined RL codebase development.

## Features

*   A common [interface for environments](https://github.com/pytorch/rl/blob/main/torchrl/envs) which supports common libraries (OpenAI gym, deepmind control lab, etc.) and state-less execution.
*   Multiprocess and distributed [data collectors](https://github.com/pytorch/rl/blob/main/torchrl/collectors/collectors.py).
*   Efficient and generic [replay buffers](https://github.com/pytorch/rl/blob/main/torchrl/data/replay_buffers/replay_buffers.py) with modularized storage.
*   Cross-library [environment transforms](https://github.com/pytorch/rl/blob/main/torchrl/envs/transforms/transforms.py).
*   Various architectures and models.
*   Exploration wrappers and modules.
*   Efficient loss modules and functional return and advantage computation.
*   A generic [trainer class](https://github.com/pytorch/rl/blob/main/torchrl/trainers/trainers.py) and various recipes.

## Examples, Tutorials, and Demos

Explore [State-of-the-Art implementations](https://github.com/pytorch/rl/blob/main/sota-implementations/), [code examples](examples/), and [tutorials and demos](https://pytorch.org/rl/stable#tutorials).

## Citation

If you use TorchRL, please cite the following:

```
@misc{bou2023torchrl,
      title={TorchRL: A data-driven decision-making library for PyTorch}, 
      author={Albert Bou and Matteo Bettini and Sebastian Dittert and Vikash Kumar and Shagun Sodhani and Xiaomeng Yang and Gianni De Fabritiis and Vincent Moens},
      year={2023},
      eprint={2306.00577},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```

## Installation

### Create a new virtual environment:
```bash
python -m venv torchrl
source torchrl/bin/activate  # On Windows use: venv\Scripts\activate
```

Or create a conda environment where the packages will be installed.

```
conda create --name torchrl python=3.9
conda activate torchrl
```

### Install dependencies:

#### PyTorch

See [here](https://pytorch.org/get-started/locally/) for installation.

#### Torchrl

```bash
pip3 install torchrl
```

The **nightly build** can be installed via
```bash
pip3 install tensordict-nightly torchrl-nightly
```

## Asking a Question

For bugs, please raise an issue in this repo.

For more general questions, use the [PyTorch forum](https://discuss.pytorch.org/c/reinforcement-learning/6).

## Contributing

Contributions are welcome!  See the [contribution guide](https://github.com/pytorch/rl/blob/main/CONTRIBUTING.md) for details.  A list of open contributions is found [here](https://github.com/pytorch/rl/issues/509). Install [pre-commit hooks](https://pre-commit.com/) for linting.

## Disclaimer

This is a beta feature, and breaking changes are possible.

## License

TorchRL is licensed under the MIT License. See [LICENSE](https://github.com/pytorch/rl/blob/main/LICENSE) for details.