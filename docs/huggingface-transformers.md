<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">
    <img alt="Hugging Face Transformers Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" width="352" height="59" style="max-width: 100%;">
  </picture>
  <br/>
  <br/>
</p>

<p align="center">
    <a href="https://huggingface.com/models"><img alt="Checkpoints on Hub" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen"></a>
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>

<h4 align="center">
    <p>
        <b>English</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">ÁÆÄ‰Ωì‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">ÁπÅÈ´î‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">ÌïúÍµ≠Ïñ¥</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">Espa√±ol</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">Êó•Êú¨Ë™û</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">–†—É—Å—Å–∫–∏–π</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">Portugu√™s</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">Fran√ßais</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Ti·∫øng Vi·ªát</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">ÿßÿ±ÿØŸà</a> |
    </p>
</h4>

## Hugging Face Transformers: State-of-the-Art Models for NLP, Computer Vision, and More

**Harness the power of cutting-edge machine learning with the Hugging Face Transformers library, enabling you to easily implement and experiment with the latest advancements in AI.**

### Key Features:

*   **Wide Range of Models:** Access over 1 million pre-trained models for a variety of tasks across text, computer vision, audio, video, and multimodal applications.
*   **Unified API:** Use a simple, consistent API for all models, making it easy to switch between architectures and experiment.
*   **Simplified Training & Inference:** Train and deploy state-of-the-art models with minimal code, reducing compute costs and accelerating development.
*   **Framework Flexibility:** Seamlessly integrate models with popular frameworks like PyTorch, TensorFlow, and Flax, choosing the best tools for each stage of your project.
*   **Community-Driven:** Benefit from a vibrant community of developers and researchers, with extensive documentation, examples, and support.

### Get Started

Use the [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API for quick and easy inference. Here's how to get started:

```python
from transformers import pipeline

# Text Generation Example
generator = pipeline("text-generation", model="Qwen/Qwen2.5-1.5B")
result = generator("the secret to baking a really good cake is ")
print(result[0]["generated_text"])

# Chatbot Example
import torch
chat = [
    {"role": "system", "content": "You are a sassy, wise-cracking robot."},
    {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
]

pipeline = pipeline(task="text-generation", model="meta-llama/Meta-Llama-3-8B-Instruct", dtype=torch.bfloat16, device_map="auto")
response = pipeline(chat, max_new_tokens=512)
print(response[0]["generated_text"][-1]["content"])
```

Explore the [Hugging Face Hub](https://huggingface.co/models) to find and experiment with models.

### Installation

Transformers works with Python 3.9+ and PyTorch 2.1+, TensorFlow 2.6+, and Flax 0.4.1+.

1.  **Create a virtual environment:**  Use [venv](https://docs.python.org/3/library/venv.html) or [uv](https://docs.astral.sh/uv/).

    ```bash
    # venv
    python -m venv .my-env
    source .my-env/bin/activate

    # uv
    uv venv .my-env
    source .my-env/bin/activate
    ```

2.  **Install Transformers:**

    ```bash
    # pip
    pip install "transformers[torch]"

    # uv
    uv pip install "transformers[torch]"
    ```

    To install from source:

    ```bash
    git clone https://github.com/huggingface/transformers.git
    cd transformers
    pip install .[torch]  # or uv pip install .[torch]
    ```

### Why Use Transformers?

*   **Simplified AI:**  Ease of use lowers the barrier to entry for AI, allowing developers of all skill levels to get involved.
*   **Efficiency:** Reduce computational costs and carbon footprint by utilizing pre-trained models and shared resources.
*   **Flexibility:** Train, evaluate, and deploy models across multiple frameworks.
*   **Customization:** Adapt models to your specific needs with ease.

### Limitations

*   **Not a modular toolbox:** The focus is on enabling research and rapid iteration on existing models.
*   **Training API optimization:**  The training API is designed for PyTorch models provided by Transformers.
*   **Example scripts:**  Adapt them to your specific use case.

### 100+ Projects Built with Transformers

Join the community and explore the [awesome-transformers](./awesome-transformers.md) page to discover a wide range of projects using Transformers.

### Example Models

A small selection of examples from different modalities. Explore their respective [model pages](https://huggingface.co/models) to try out models directly.

**Audio**

*   Audio classification: [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)
*   Automatic speech recognition: [Moonshine](https://huggingface.co/UsefulSensors/moonshine)
*   Keyword spotting: [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)
*   Speech to speech generation: [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)
*   Text to audio: [MusicGen](https://huggingface.co/facebook/musicgen-large)
*   Text to speech: [Bark](https://huggingface.co/suno/bark)

**Computer Vision**

*   Automatic mask generation: [SAM](https://huggingface.co/facebook/sam-vit-base)
*   Depth estimation: [DepthPro](https://huggingface.co/apple/DepthPro-hf)
*   Image classification: [DINO v2](https://huggingface.co/facebook/dinov2-base)
*   Keypoint detection: [SuperPoint](https://huggingface.co/magic-leap-community/superpoint)
*   Keypoint matching: [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)
*   Object detection: [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)
*   Pose Estimation: [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)
*   Universal segmentation: [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)
*   Video classification: [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)

**Multimodal**

*   Audio or text to text: [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)
*   Document question answering: [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)
*   Image or text to text: [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
*   Image captioning [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)
*   OCR-based document understanding: [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)
*   Table question answering: [TAPAS](https://huggingface.co/google/tapas-base)
*   Unified multimodal understanding and generation: [Emu3](https://huggingface.co/BAAI/Emu3-Gen)
*   Vision to text: [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)
*   Visual question answering: [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)
*   Visual referring expression segmentation: [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)

**NLP**

*   Masked word completion: [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)
*   Named entity recognition: [Gemma](https://huggingface.co/google/gemma-2-2b)
*   Question answering: [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)
*   Summarization: [BART](https://huggingface.co/facebook/bart-large-cnn)
*   Translation: [T5](https://huggingface.co/google-t5/t5-base)
*   Text generation: [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)
*   Text classification: [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)

### Citation

If you use the ü§ó Transformers library, please cite the following paper:

```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R√©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
```

[Back to the top](https://github.com/huggingface/transformers)