<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg">
    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">
    <img alt="Hugging Face Transformers Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" width="352" height="59" style="max-width: 100%;">
  </picture>
  <br/>
  <br/>
</p>

<p align="center">
    <a href="https://huggingface.com/models"><img alt="Checkpoints on Hub" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen"></a>
    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>
    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>
    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>
    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>
    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>
</p>

<h4 align="center">
    <p>
        <b>English</b> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">ÁÆÄ‰Ωì‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">ÁπÅÈ´î‰∏≠Êñá</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">ÌïúÍµ≠Ïñ¥</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">Espa√±ol</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">Êó•Êú¨Ë™û</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">–†—É—Å—Å–∫–∏–π</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">Portugu√™s</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">Fran√ßais</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Ti·∫øng Vi·ªát</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a> |
        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">ÿßÿ±ÿØŸà</a> |
    </p>
</h4>

<h2 align="center">üöÄ Hugging Face Transformers: State-of-the-Art Models for NLP, Computer Vision, and More </h2>

<p align="center">
    <b>Hugging Face Transformers</b> empowers you to easily use and customize pre-trained models for a wide range of AI tasks.
</p>

<h3 align="center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png"/>
</h3>

Hugging Face Transformers is a comprehensive library providing pre-trained models for a multitude of AI applications, including text, computer vision, audio, video, and multimodal tasks.  It serves as a central framework for model definitions, ensuring consistency across the AI ecosystem and compatibility with various training frameworks, inference engines, and adjacent libraries.  

**Key Features:**

*   **Extensive Model Support**: Access to over 1 million pre-trained model checkpoints on the [Hugging Face Hub](https://huggingface.co/models?library=transformers&sort=trending).
*   **Unified API**:  A consistent and easy-to-use API for all pre-trained models.
*   **Cross-Framework Compatibility**: Works seamlessly with popular frameworks like PyTorch, TensorFlow, and Flax, supporting a wide range of training and inference tools.
*   **Customization and Flexibility**:  Easily adapt models and examples to meet your specific requirements.
*   **Efficiency**: Leverage pre-trained models to reduce compute costs and accelerate development.
*   **Community Driven**: Benefit from a thriving community and contribute to the growing ecosystem.

Explore the [Hugging Face Hub](https://huggingface.co/) to find models and get started quickly.

## Installation

Transformers requires Python 3.9+ and supports PyTorch 2.1+, TensorFlow 2.6+, and Flax 0.4.1+.

**Installation Steps:**

1.  **Create a Virtual Environment**: Use `venv` or `uv` to set up an isolated environment.

    ```bash
    # venv
    python -m venv .my-env
    source .my-env/bin/activate
    # uv
    uv venv .my-env
    source .my-env/bin/activate
    ```

2.  **Install Transformers**: Install the library using `pip` or `uv`.

    ```bash
    # pip
    pip install "transformers[torch]"

    # uv
    uv pip install "transformers[torch]"
    ```

3.  **Install from Source (for latest changes)**:  Clone the repository and install.

    ```bash
    git clone https://github.com/huggingface/transformers.git
    cd transformers

    # pip
    pip install .[torch]

    # uv
    uv pip install .[torch]
    ```

## Quickstart

Get up and running with Transformers quickly using the [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API, a high-level inference tool for various tasks.

**Example: Text Generation**

```python
from transformers import pipeline

pipeline = pipeline(task="text-generation", model="Qwen/Qwen2.5-1.5B")
pipeline("the secret to baking a really good cake is ")
```

**Chatbot Example:**

```python
import torch
from transformers import pipeline

chat = [
    {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
    {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
]

pipeline = pipeline(task="text-generation", model="meta-llama/Meta-Llama-3-8B-Instruct", torch_dtype=torch.bfloat16, device_map="auto")
response = pipeline(chat, max_new_tokens=512)
print(response[0]["generated_text"][-1]["content"])
```

Explore the expanded examples below to see how `Pipeline` works for diverse modalities and tasks.

<details>
<summary>Automatic speech recognition</summary>

```python
from transformers import pipeline

pipeline = pipeline(task="automatic-speech-recognition", model="openai/whisper-large-v3")
pipeline("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
```

</details>

<details>
<summary>Image classification</summary>

<h3 align="center">
    <a><img src="https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"></a>
</h3>

```python
from transformers import pipeline

pipeline = pipeline(task="image-classification", model="facebook/dinov2-small-imagenet1k-1-layer")
pipeline("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")
```

</details>

<details>
<summary>Visual question answering</summary>


<h3 align="center">
    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg"></a>
</h3>

```python
from transformers import pipeline

pipeline = pipeline(task="visual-question-answering", model="Salesforce/blip-vqa-base")
pipeline(
    image="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg",
    question="What is in the image?",
)
```

</details>

## Why Use Transformers?

*   **Ease of Use**:  Simple, state-of-the-art models that are easy to implement.
*   **Cost-Effectiveness**:  Reduce expenses and environmental impact by utilizing pre-trained models.
*   **Framework Flexibility**: Choose the optimal framework for your needs.
*   **Customization**: Easily tailor models to your specific projects.

## Considerations

*   **Not a Modular Toolbox**: The library focuses on providing the model definitions and is not structured as a modular building blocks library.
*   **Training API Limitations**: The training API is optimized for PyTorch models within Transformers.
*   **Example Scripts**:  Adapt example scripts to fit your individual use cases.

<a target="_blank" href="https://huggingface.co/enterprise">
    <img alt="Hugging Face Enterprise Hub" src="https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925">
</a><br>

## Featured Projects

Explore the [awesome-transformers](./awesome-transformers.md) page, highlighting 100 incredible projects built with Transformers and the Hugging Face Hub.

## Example Models

Test a variety of models directly on their [Hub model pages](https://huggingface.co/models).

<details>
<summary>Audio</summary>
- Audio classification with [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)
- Automatic speech recognition with [Moonshine](https://huggingface.co/UsefulSensors/moonshine)
- Keyword spotting with [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)
- Speech to speech generation with [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)
- Text to audio with [MusicGen](https://huggingface.co/facebook/musicgen-large)
- Text to speech with [Bark](https://huggingface.co/suno/bark)
</details>

<details>
<summary>Computer vision</summary>
- Automatic mask generation with [SAM](https://huggingface.co/facebook/sam-vit-base)
- Depth estimation with [DepthPro](https://huggingface.co/apple/DepthPro-hf)
- Image classification with [DINO v2](https://huggingface.co/facebook/dinov2-base)
- Keypoint detection with [SuperPoint](https://huggingface.co/magic-leap-community/superpoint)
- Keypoint matching with [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)
- Object detection with [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)
- Pose Estimation with [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)
- Universal segmentation with [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)
- Video classification with [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)
</details>

<details>
<summary>Multimodal</summary>
- Audio or text to text with [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)
- Document question answering with [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)
- Image or text to text with [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
- Image captioning [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)
- OCR-based document understanding with [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)
- Table question answering with [TAPAS](https://huggingface.co/google/tapas-base)
- Unified multimodal understanding and generation with [Emu3](https://huggingface.co/BAAI/Emu3-Gen)
- Vision to text with [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)
- Visual question answering with [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)
- Visual referring expression segmentation with [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)
</details>

<details>
<summary>NLP</summary>
- Masked word completion with [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)
- Named entity recognition with [Gemma](https://huggingface.co/google/gemma-2-2b)
- Question answering with [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)
- Summarization with [BART](https://huggingface.co/facebook/bart-large-cnn)
- Translation with [T5](https://huggingface.co/google-t5/t5-base)
- Text generation with [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)
- Text classification with [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)
</details>

## Citation

For research, please cite the Transformers library:
```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R√©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
```
Find the source code on [GitHub](https://github.com/huggingface/transformers).