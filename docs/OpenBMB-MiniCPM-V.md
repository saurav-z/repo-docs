# MiniCPM-V: The On-Device Multimodal AI Revolution - Open Source!

**Unlock the power of GPT-4o-level multimodal understanding on your phone with MiniCPM-V, a cutting-edge series of efficient and open-source Multimodal Large Language Models (MLLMs).** [Explore the original repository](https://github.com/OpenBMB/MiniCPM-V).

<div align="center">
  <img src="./assets/minicpm_v_and_minicpm_o_title.png" width="500em" >
</div>

## Key Features

*   **Unmatched On-Device Performance:** MiniCPM-V models are specifically designed for efficient deployment on mobile devices, offering exceptional performance in vision and audio-language tasks.
*   **Multimodal Input Support:** Process images, videos, text, and audio (MiniCPM-o) to generate rich and contextually relevant text outputs.
*   **GPT-4o-Level Capability:** Achieve state-of-the-art results in vision-language tasks, rivaling and often surpassing leading proprietary models like GPT-4o, Gemini, and Claude.
*   **High-FPS and Long Video Understanding:** Analyze high-frame-rate and extended video content with exceptional efficiency.
*   **Advanced Features:** Benefit from features such as strong OCR, document parsing, multilingual support, and trustworthy behavior based on cutting edge research, including techniques such as RLAIF-V, VisCPM, and LLaVA-UHD.
*   **Easy Deployment:** Deploy and utilize MiniCPM-V models using a wide array of options, including llama.cpp, Ollama, vLLM, and Transformers.  Experiment with both quantized and full precision models.

## Available Models

*   **MiniCPM-V 4.5:** The latest and most advanced model in the MiniCPM-V series, achieving state-of-the-art vision-language performance with 8B parameters. Outperforms models such as GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B in vision-language capabilities. Try it now with [ü§ó](https://huggingface.co/openbmb/MiniCPM-V-4_5).
*   **MiniCPM-o 2.6:** An end-to-end multimodal model that handles images, text, and audio.  Achieves GPT-4o-level performance in vision, speech, and multimodal live streaming. Try it now with [ü§ó](https://huggingface.co/openbmb/MiniCPM-o-2_6).

## Updates & News

*   **[2024.08.26]** üî•üî•üî• We open-source MiniCPM-V 4.5, which outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B. It advances popular capabilities of MiniCPM-V, and brings useful new features. Try it now!
*   **[2025.09.01]** ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è MiniCPM-V 4.5 has been officially supported by [llama.cpp](https://github.com/ggml-org/llama.cpp/pull/15575), [vLLM](https://github.com/vllm-project/vllm/pull/23586), and [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory/pull/9022).

[Click here to view all the news.](#news)

## Get Started

*   **[MiniCPM-V & o Cookbook](https://github.com/OpenSQZ/MiniCPM-V-CookBook):** Explore comprehensive guides for a variety of use cases, including deployments on Ollama, Llama.cpp, and vLLM.
*   **Web Demos:** Interact with the models via online demos, including [MiniCPM-o 2.6](https://minicpm-omni-webdemo-us.modelbest.cn/) | [MiniCPM-V 2.6](http://120.92.209.146:8887/) | [MiniCPM-Llama3-V 2.5](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5) | [MiniCPM-V 2.0](https://huggingface.co/spaces/openbmb/MiniCPM-V-2).
*   **Model Zoo:** Browse the available models and their download links.  \[[Model Zoo](#model-zoo)]
*   **Code Examples:** Start with example code for multi-turn conversations, multi-image input, video processing, and audio-language tasks. \[[Inference](#inference)]
*   **Fine-tuning:** Learn how to fine-tune MiniCPM-V models with Hugging Face and other frameworks. \[[Fine-tuning](#fine-tuning)]
## Inference
  *   **Model Zoo:** Find the right model for your needs
  *   **Multi-turn Conversation:**  Run conversational threads with ease.
  *   **Inference on Multiple GPUs:** Optimize performance using multiple GPUs.
  *   **Inference on Mac:** Run on Apple silicon with MPS.
  *   **Efficient Inference with llama.cpp, Ollama, vLLM:** Benefit from a diverse range of deployment methods for your specific environment.

## Additional Resources

*   **FAQs:** Get answers to common questions.  \[[FAQs](#faqs)]
*   **Limitations:** Be aware of the models' constraints.  \[[Limitations](#limitations)]
*   **Citation:** Cite our research.  \[[Citation](#citation)]
*   **Model License:** Review the licensing terms for the models.
*   **Key Techniques and Other Projects:** Explore core techniques of MiniCPM-o/V

**Join the future of on-device multimodal AI ‚Äì explore MiniCPM-V today!**