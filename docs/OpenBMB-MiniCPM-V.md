<div align="center">

<img src="./assets/minicpm_v_and_minicpm_o_title.png" width="500em" ></img> 

</div>

# MiniCPM-V & MiniCPM-o: Powerful Multimodal LLMs for On-Device Understanding

**MiniCPM-V and MiniCPM-o are a series of cutting-edge multimodal large language models (MLLMs) designed for efficient, on-device deployment, providing high-quality text and speech outputs from visual, audio, and textual inputs.** Learn more about the models, find out how to use them, and get involved by visiting the original repository on [GitHub](https://github.com/OpenBMB/MiniCPM-V).

## Key Features:

*   **Vision & Language Mastery:** Understand images, videos, and text to generate high-quality text responses.
*   **Audio & Speech Integration (MiniCPM-o):** Process audio inputs and generate high-quality speech outputs for enhanced interaction.
*   **On-Device Efficiency:** Optimized for efficient deployment on your phone and other edge devices.
*   **Multilingual Support:**  Support for over 30 languages, expanding accessibility.
*   **Real-time Performance:** Achieve high FPS video understanding and support for multimodal live streaming capabilities.

## Key Highlights:

*   **MiniCPM-V 4.5:** The latest, most capable model in the MiniCPM-V series. Outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B in vision-language capabilities, making it the most performant on-device multimodal model in the open-source community. Key features include:
    *   High-FPS and long video understanding.
    *   Controllable hybrid fast/deep thinking.
    *   Robust handwritten OCR and complex table/document parsing.
*   **MiniCPM-o 2.6:** Offers comparable performance to GPT-4o-202405 in vision, speech, and multimodal live streaming, and also supports end-to-end multimodal live streaming on end-side devices. Features include:
    *   Bilingual real-time speech conversation with configurable voices.
    *   Voice cloning and role-playing capabilities.

## Quick Links:

*   [**MiniCPM-V 4.5**](https://huggingface.co/openbmb/MiniCPM-V-4_5)
*   [**MiniCPM-o 2.6**](https://huggingface.co/openbmb/MiniCPM-o-2_6)
*   [**Cookbook**](https://github.com/OpenSQZ/MiniCPM-V-Cookbook) for in-depth guides and deployment options.
*   [**Technical Report (MiniCPM-V 4.5)**](https://github.com/OpenBMB/MiniCPM-V/blob/main/docs/MiniCPM_V_4_5_Technical_Report.pdf)
*   [**Technical Report (MiniCPM-o 2.6)**](https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9)

## Inference

*   **Model Zoo:** A curated list of available models.
*   **Multi-turn Conversation:** Interact with the models in engaging, multi-turn dialogues.
*   **Inference with multiple images, Few-shot Learning, video, and multimodal live streaming.**
*   **Inference on Multiple GPUs:** Instructions on running the models efficiently on multi-GPU setups.
*   **Inference on Mac:** Guidance for running on macOS.
*   **Efficient Inference:** Resources for efficient deployment using llama.cpp, Ollama, and vLLM.

## Further Information:

*   [**Fine-tuning**](#fine-tuning): Learn how to fine-tune the models.
*   [**Awesome work using MiniCPM-V & MiniCPM-o**](#awesome-work-using-minicpm-v--minicpm-o): Explore examples of projects built using the models.
*   [**FAQs**](#faqs): Find answers to frequently asked questions.
*   [**Limitations**](#limitations): Understand model limitations.
*   [**Model License**](#model-license)
*   [**Citation**](#citation)