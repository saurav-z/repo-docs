# Hunyuan-MT: State-of-the-Art Multilingual Translation Models

**Hunyuan-MT offers superior multilingual translation capabilities, achieving top performance and supporting 33 languages, including minority languages.** [Explore the Hunyuan-MT Repository](https://github.com/Tencent-Hunyuan/Hunyuan-MT).

---

## Key Features and Advantages:

*   ğŸ¥‡ **Top Performance:** Achieved first place in 30 out of 31 language categories in the WMT25 competition.
*   ğŸš€ **Industry-Leading:** Hunyuan-MT-7B delivers exceptional performance compared to similarly sized models.
*   ğŸ§  **Ensemble Model:** Hunyuan-MT-Chimera-7B is the first open-source translation ensemble model, enhancing translation quality.
*   âš™ï¸ **Comprehensive Training:** Implements a robust training pipeline: pretrain â†’ CPT â†’ SFT â†’ translation RL â†’ ensemble RL, leading to SOTA results.
*   ğŸŒ **Multilingual Support:** Translates between 33 languages, including Chinese and five Chinese minority languages.
*   âš¡ **Quantization and Deployment options**: The models can be deployed via TensorRT-LLM, vLLM, or SGLang.

---

## Model Details and Resources

### Model Links

| Model Name | Description | Download |
| ----------- | ----------- | ----------- |
| Hunyuan-MT-7B | Hunyuan 7B translation model | ğŸ¤— [Model](https://huggingface.co/tencent/Hunyuan-MT-7B) |
| Hunyuan-MT-7B-fp8 | Hunyuan 7B translation modelï¼Œfp8 quant | ğŸ¤— [Model](https://huggingface.co/tencent/Hunyuan-MT-7B-fp8) |
| Hunyuan-MT-Chimera | Hunyuan 7B translation ensemble model | ğŸ¤— [Model](https://huggingface.co/tencent/Hunyuan-MT-Chimera-7B) |
| Hunyuan-MT-Chimera-fp8 | Hunyuan 7B translation ensemble modelï¼Œfp8 quant | ğŸ¤— [Model](https://huggingface.co/tencent/Hunyuan-MT-Chimera-7B-fp8) |

### Prompts

#### Prompt Template for ZH<=>XX Translation:

```
æŠŠä¸‹é¢çš„æ–‡æœ¬ç¿»è¯‘æˆ<target_language>ï¼Œä¸è¦é¢å¤–è§£é‡Šã€‚

<source_text>
```

#### Prompt Template for XX<=>XX Translation, excluding ZH<=>XX:

```
Translate the following segment into <target_language>, without additional explanation.

<source_text>
```

#### Prompt Template for Hunyuan-MT-Chimera-7B:

```
Analyze the following multiple <target_language> translations of the <source_language> segment surrounded in triple backticks and generate a single refined <target_language> translation. Only output the refined translation, do not explain.

The <source_language> segment:
```<source_text>```

The multiple `<target_language>` translations:
1. ```<translated_text1>```
2. ```<translated_text2>```
3. ```<translated_text3>```
4. ```<translated_text4>```
5. ```<translated_text5>```
6. ```<translated_text6>```
```

### Supported Languages

| Languages | Abbr. | Chinese Names |
|---|---|---|
| Chinese | zh | ä¸­æ–‡ |
| English | en | è‹±è¯­ |
| French | fr | æ³•è¯­ |
| Portuguese | pt | è‘¡è„ç‰™è¯­ |
| Spanish | es | è¥¿ç­ç‰™è¯­ |
| Japanese | ja | æ—¥è¯­ |
| Turkish | tr | åœŸè€³å…¶è¯­ |
| Russian | ru | ä¿„è¯­ |
| Arabic | ar | é˜¿æ‹‰ä¼¯è¯­ |
| Korean | ko | éŸ©è¯­ |
| Thai | th | æ³°è¯­ |
| Italian | it | æ„å¤§åˆ©è¯­ |
| German | de | å¾·è¯­ |
| Vietnamese | vi | è¶Šå—è¯­ |
| Malay | ms | é©¬æ¥è¯­ |
| Indonesian | id | å°å°¼è¯­ |
| Filipino | tl | è²å¾‹å®¾è¯­ |
| Hindi | hi | å°åœ°è¯­ |
| Traditional Chinese | zh-Hant | ç¹ä½“ä¸­æ–‡ |
| Polish | pl | æ³¢å…°è¯­ |
| Czech | cs | æ·å…‹è¯­ |
| Dutch | nl | è·å…°è¯­ |
| Khmer | km | é«˜æ£‰è¯­ |
| Burmese | my | ç¼…ç”¸è¯­ |
| Persian | fa | æ³¢æ–¯è¯­ |
| Gujarati | gu | å¤å‰æ‹‰ç‰¹è¯­ |
| Urdu | ur | ä¹Œå°”éƒ½è¯­ |
| Telugu | te | æ³°å¢å›ºè¯­ |
| Marathi | mr | é©¬æ‹‰åœ°è¯­ |
| Hebrew | he | å¸Œä¼¯æ¥è¯­ |
| Bengali | bn | å­ŸåŠ æ‹‰è¯­ |
| Tamil | ta | æ³°ç±³å°”è¯­ |
| Ukrainian | uk | ä¹Œå…‹å…°è¯­ |
| Tibetan | bo | è—è¯­ |
| Kazakh | kk | å“ˆè¨å…‹è¯­ |
| Mongolian | mn | è’™å¤è¯­ |
| Uyghur | ug | ç»´å¾å°”è¯­ |
| Cantonese | yue | ç²¤è¯­ |

### Usage and Deployment

*   **Transformers Library:** Integrate Hunyuan-MT using the `transformers` library. See the example code in the original README for details.
*   **Inference Parameters:** Recommended inference parameters are included.
*   **Training Data Format:** Guidance on data format for fine-tuning the model.
*   **Training with LLaMA-Factory:** Step-by-step instructions for fine-tuning with LLaMA-Factory, including prerequisites, data preparation, and execution commands.
*   **Quantization Compression:** Learn about using FP8 and INT4 quantization, with links to quantization models and the AngelSlim compression tool.
*   **Deployment Options:**
    *   **TensorRT-LLM:**  Provides a Docker image and configuration examples for deployment.
    *   **vLLM:**  Instructions for deploying with vLLM, including example API server setups and quantitative model deployment.
    *   **SGLang:** Includes details on launching a server with SGLang and using a Docker image.

---

## Example Code (Transformers)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name_or_path = "tencent/Hunyuan-MT-7B"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map="auto")
messages = [
    {"role": "user", "content": "Translate the following segment into Chinese, without additional explanation.\n\nItâ€™s on the house."},
]
tokenized_chat = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=False,
    return_tensors="pt"
)

outputs = model.generate(tokenized_chat.to(model.device), max_new_tokens=2048)
output_text = tokenizer.decode(outputs[0])
```

---

## Contact Us

For questions or feedback, contact the open-source team or send an email to hunyuan\_opensource@tencent.com.

```bibtex
@misc{hunyuan_mt,
      title={Hunyuan-MT Technical Report},
      author={Mao Zheng and Zheng Li and Bingxin Qu and Mingyang Song and Yang Du and Mingrui Sun and Di Wang},
      year={2025},
      eprint={2509.05209},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2509.05209},
}