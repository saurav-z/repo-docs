<!--
SPDX-FileCopyrightText: 2024, the R-Zero Authors
SPDX-License-Identifier: BSD-3-Clause
-->

# R-Zero: Revolutionizing Reasoning in LLMs with Self-Evolving AI

**R-Zero empowers Large Language Models (LLMs) to autonomously develop reasoning abilities, eliminating the need for pre-existing datasets or manual labeling.** For detailed information, explore our [research paper](https://arxiv.org/abs/2508.05004) and [project webpage](https://chengsong-huang.github.io/R-Zero.github.io/). ([Back to original repo](https://github.com/Chengsong-Huang/R-Zero))

## Key Features

*   üöÄ **Zero-Shot Learning:** R-Zero begins with a base model and evolves its reasoning capabilities without any external data.
*   üîÑ **Co-Evolutionary Loop:**  A "Challenger" model probes a "Solver" model, creating an adaptive learning curriculum for continuous improvement.
*   üìà **Performance Boost:**  Achieves significant performance gains across various reasoning benchmarks.
*   üåç **Generalization:** Reasoning skills learned in specific domains effectively transfer to broader reasoning tasks.
*   üß† **Model-Agnostic:**  Enhances the reasoning performance of diverse base LLMs.

## What's New
*   **2025-08-25:** Updated code to improve training stability (by stopit).
*   **2025-08-08:**  Recognized as `#2 Paper of the day` on [Hugging Face Daily Papers](https://huggingface.co/papers/2508.05004).
*   **2025-08-07:**  Released our [research paper](https://arxiv.org/abs/2508.05004) and associated code.

## Overview

[![R-Zero Overview](figs/abstract.png)](https://arxiv.org/abs/2508.05004)

Training advanced reasoning models typically relies on extensive, manually curated datasets, which are costly and difficult to scale. R-Zero offers a novel framework that enables LLMs to autonomously refine their reasoning skills, eliminating the need for pre-existing tasks or labels, by learning from scratch.

At the core of R-Zero is a dynamic co-evolutionary loop between two instances of the same base model:

1.  **Challenger üéØ:** Designed to identify weaknesses in the Solver and generate challenging problems aligned with its capabilities.
2.  **Solver üß†:** Focuses on continuous improvement by solving the increasingly complex tasks generated by the Challenger.

This process constructs a perfectly tailored, adaptive curriculum. The Challenger learns to ask more insightful questions, while the Solver learns to generate better answers. The entire cycle is self-contained, leveraging techniques like majority voting for pseudo-labels and relative policy optimization to guide the learning process.

## Quickstart Guide

Get started with R-Zero in a few simple steps:

### 1.  Environment Setup and Directory Preparation

```bash
git clone https://github.com/Chengsong-Huang/R-Zero.git
cd R-Zero
pip install -r requirements.txt
export STORAGE_PATH="/path/to/your/storage"  # Set a path for storing checkpoints and data.
export HUGGINGFACENAME="yourhuggingfacename"
mkdir -p "$STORAGE_PATH/evaluation" "$STORAGE_PATH/models" "$STORAGE_PATH/generated_question" "$STORAGE_PATH/temp_results"
```

### 2.  API Key Configuration

*   Add your API keys for **Hugging Face** and **WandB** (for logging) in `tokens.json`.
*   Include your **OpenAI GPT** API key in `evaluation/results_recheck.py` for evaluation purposes.

### 3. Run Experiments

Replicate our experimental results using the following script:

```bash
# Run the script with the base model name and an abbreviation as arguments
# The abbreviation generates a directory for model storage.
# Example: bash scripts/main.sh Qwen/Qwen3-4B-Base qwen3-4b
bash scripts/main.sh Qwen/Qwen3-4B-Base qwen3-4b
```

## Impressive Results

The table below compares the performance of the Base Model, a Zero-Shot Challenger baseline, and our iterative R-Zero framework. Peak performance for each model is highlighted in **bold**.

| Model Name | Overall AVG | MATH AVG | SuperGPQA | MMLU-Pro | BBEH |
|:---|:---:|:---:|:---:|:---:|:---:|
| ***Qwen3-4B-Base*** | | | | | |
| &emsp;Base Model | 27.10 | 42.58 | 20.88 | 37.38 | 7.57 |
| &emsp;Base Challenger | 30.83 | 44.36 | 24.77 | 47.59 | 6.59 |
| &emsp;R-Zero (Iter 1) | 34.27 | 48.06 | **27.92** | 51.69 | 9.42 |
| &emsp;R-Zero (Iter 2) | **34.92** | 48.44 | 27.72 | **53.75** | 9.76 |
| &emsp;R-Zero (Iter 3) | 34.64 | **49.07** | 27.55 | 51.53 | **10.42** |
| ***Qwen3-8B-Base*** | | | | | |
| &emsp;Base Model | 34.49 | 49.18 | 28.33 | 51.80 | 8.63 |
| &emsp;Base Challenger | 36.43 | 51.87 | 30.12 | 54.14 | 9.60 |
| &emsp;R-Zero (Iter 1) | 37.93 | 53.39 | 31.26 | 57.17 | 9.91 |
| &emsp;R-Zero (Iter 2) | 38.45 | 53.84 | **31.58** | 58.20 | 10.20 |
| &emsp;R-Zero (Iter 3) | **38.73** | **54.69** | 31.38 | **58.23** | **10.60** |
| ***OctoThinker-3B*** | | | | | |
| &emsp;Base Model | 12.27 | 26.64 | 10.09 | 10.87 | 1.46 |
| &emsp;Base Challenger | 14.41 | 27.51 | 11.19 | 14.53 | **4.40** |
| &emsp;R-Zero (Iter 1) | 14.93 | 27.76 | 12.21 | 15.72 | 4.05 |
| &emsp;R-Zero (Iter 2) | 15.11 | 28.20 | 12.43 | 16.08 | 3.74 |
| &emsp;R-Zero (Iter 3) | **15.67** | **29.32** | **12.44** | **16.71** | 4.20 |
| ***OctoThinker-8B*** | | | | | |
| &emsp;Base Model | 16.81 | 32.11 | 13.26 | 20.21 | 1.64 |
| &emsp;Base Challenger | 25.08 | 36.41 | 16.99 | 41.46 | 5.46 |
| &emsp;R-Zero (Iter 1) | 26.44 | 37.80 | 19.15 | **42.05** | 6.77 |
| &emsp;R-Zero (Iter 2) | 26.77 | 38.23 | 19.27 | 41.34 | **8.25** |
| &emsp;R-Zero (Iter 3) | **26.88** | **38.52** | **19.82** | 40.92 | **8.25** |

## FAQ for Developers

### Q: What hardware setup was used for these experiments?

**A:** All experiments were performed on an 8-GPU server using models that can run on a single GPU (4B or 8B). If you need to run with larger models or different hardware, you will have to modify the code.

### Q: How do I troubleshoot environment configuration issues?

**A:** Our framework is based on [EasyR1](https://github.com/hiyouga/EasyR1/tree/main). Consult their setup instructions or use their Docker environment for help.

### Q: Where are training logs and model checkpoints saved?

**A:** All generated data, including logs, datasets, and model checkpoints, are saved in the directory specified by the `STORAGE_PATH` environment variable. The dataset will also be sent to Hugging Face via `HUGGINGFACENAME`.

### Q: What if the code gets stuck during the questioner training process?

**A:**  This can be due to a bug in the `math_verify` library. Implement a timeout to restart training from the last saved checkpoint if this occurs.

## Acknowledgements

We extend our gratitude to the authors of [**EasyR1**](https://github.com/hiyouga/EasyR1/tree/main), whose work provided the basis for our framework's core functionalities.  We also acknowledge the contributions from the research in [**General-Reasoner**](https://github.com/TIGER-AI-Lab/General-Reasoner) which we referenced in our evaluation process.

## Citation

If you find our work helpful, please cite our paper:

```
@article{huang2025rzeroselfevolvingreasoningllm,
      title={R-Zero: Self-Evolving Reasoning LLM from Zero Data}, 
      author={Chengsong Huang and Wenhao Yu and Xiaoyang Wang and Hongming Zhang and Zongxia Li and Ruosen Li and Jiaxin Huang and Haitao Mi and Dong Yu},
      year={2025},
      eprint={2508.05004},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2508.05004}, 
}
```

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=Chengsong-Huang/R-Zero&type=Date)](https://star-history.com/#Chengsong-Huang/R-Zero&Date)