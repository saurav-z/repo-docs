<p align="center" width="70%">
<img src="https://i.postimg.com/KvkLzbF9/WX20241212-014400-2x.png" alt="LMMs-Eval Logo">
</p>

# LMMs-Eval: Evaluate and Advance Large Multimodal Models

**Accelerate your LMM research with LMMs-Eval, a powerful and versatile evaluation suite for large multimodal models.**  ([Original Repo](https://github.com/EvolvingLMMs-Lab/lmms-eval))

[![PyPI](https://img.shields.io/pypi/v/lmms-eval)](https://pypi.org/project/lmms-eval)
![PyPI - Downloads](https://img.shields.io/pypi/dm/lmms-eval)
![GitHub contributors](https://img.shields.io/github/contributors/EvolvingLMMs-Lab/lmms-eval)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)
[![open issues](https://img.shields.io/github/issues-raw/EvolvingLMMs-Lab/lmms-eval)](https://github.com/EvolvingLMMs-Lab/lmms-eval/issues)

**Key Features:**

*   **Comprehensive Task Support:** Evaluates LMMs across a wide range of modalities, including text, image, video, and audio, with support for 100+ tasks.
*   **Extensive Model Compatibility:** Supports over 30 different LMM architectures, with new models added frequently.
*   **Flexible and Efficient Evaluation:**  Built upon the efficient design of `lm-evaluation-harness`,  offering fast and consistent evaluation.
*   **VLLM and OpenAI API Integration**: Integrated vLLM and OpenAI compatibility for accelerated and flexible model evaluation.
*   **Reproducibility Focus:** Detailed instructions and resources for reproducing results, with environment setup scripts.
*   **Active Development:** Continuously updated with new benchmarks, models, and features.
*   **Community Driven**:  Open source and actively developed, welcoming contributions and feedback.

üè† [LMMs-Lab Homepage](https://www.lmms-lab.com/) | ü§ó [Huggingface Datasets](https://huggingface.co/lmms-lab) | <a href="https://emoji.gg/emoji/1684-discord-thread"><img src="https://cdn3.emoji.gg/emojis/1684-discord-thread.png" width="14px" height="14px" alt="Discord_Thread"></a> [discord/lmms-eval](https://discord.gg/zdkwKUqrPy)

üìñ [Supported Tasks (100+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/current_tasks.md) | üåü [Supported Models (30+)](https://github.com/EvolvingLMMs-Lab/lmms-eval/tree/main/lmms_eval/models) | üìö [Documentation](docs/README.md)

---

## What's New

*   **[2025-07] lmms-eval-0.4 Release:** Major update with new features and improvements, see the [release notes](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.4.md). For older version users `lmms-eval-0.3` please refer to the branch `stable/v0d3`. Discuss model's eval results in [discussion](https://github.com/EvolvingLMMs-Lab/lmms-eval/discussions/779).
*   **[2025-07] New Task: PhyX:** A benchmark for physics-grounded reasoning in visual scenarios.
*   **[2025-06] New Task: VideoMathQA:**  Evaluates mathematical reasoning in educational videos.
*   **[2025-04] Aero-1-Audio Support:** Evaluation support for the compact audio model Aero-1-Audio, including batched evaluations.
*   **[2025-02] vLLM and OpenAI API Integration:** Accelerated evaluation with vLLM and support for any API-based model using the OpenAI API format.

**(See details below for more recent updates)**

<details>
<summary>Older Announcements</summary>

*   [2025-01] üéìüéì New benchmark: [Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos](https://arxiv.org/abs/2501.13826). Please refer to the [project page](https://videommmu.github.io/) for more details.
*   [2024-12] üéâüéâ MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs presented jointly with [MME Team](https://github.com/BradyFU/Video-MME) and [OpenCompass Team](https://github.com/open-compass).
*   [2024-11] üîàüîä Audio evaluations for audio models like Qwen2-Audio and Gemini-Audio across tasks such as AIR-Bench, Clotho-AQA, LibriSpeech, and more. Refer to the [blog](https://github.com/EvolvingLMMs-Lab/lmms-eval/blob/main/docs/lmms-eval-0.3.md).
*   [2024-10] üéâüéâ New task [NaturalBench](https://huggingface.co/datasets/BaiqiL/NaturalBench).
*   [2024-10] üéâüéâ New task [TemporalBench](https://huggingface.co/datasets/microsoft/TemporalBench).
*   [2024-10] üéâüéâ New tasks [VDC](https://rese1f.github.io/aurora-web/), [MovieChat-1K](https://rese1f.github.io/MovieChat/), and [Vinoground](https://vinoground.github.io/). New models: [AuroraCap](https://github.com/rese1f/aurora) and [MovieChat](https://rese1f.github.io/MovieChat).
*   [2024-09] üéâüéâ New tasks [MMSearch](https://mmsearch.github.io/) and [MME-RealWorld](https://mme-realworld.github.io/).
*   [2024-09] ‚öôÔ∏èÔ∏è‚öôÔ∏èÔ∏èÔ∏èÔ∏è `lmms-eval` upgraded to `0.2.3` with more tasks and features.
*   [2024-08] üéâüéâ New model [LLaVA-OneVision](https://huggingface.co/papers/2408.03326), [Mantis](https://github.com/EvolvingLMMs-Lab/lmms-eval/pull/162), new tasks [MVBench](https://huggingface.co/datasets/OpenGVLab/MVBench), [LongVideoBench](https://github.com/EvolvingLMMs-Lab/lmms-eval/pull/117), [MMStar](https://github.com/EvolvingLMMs-Lab/lmms-eval/pull/158).
*   [2024-07] üë®‚Äçüíªüë®‚Äçüíª `lmms-eval/v0.2.1` upgraded to support more models and tasks.
*   [2024-07] üéâüéâ The [technical report](https://arxiv.org/abs/2407.12772) and [LiveBench](https://huggingface.co/spaces/lmms-lab/LiveBench) released!
*   [2024-06] üé¨üé¨ `lmms-eval/v0.2.0` upgraded to support video evaluations.
*   [2024-03] üìùüìù Initial release of `lmms-eval`.

</details>

## Installation

### Recommended: Using uv for consistent environments

Follow these steps for installation using `uv`, ensuring consistent package versions across development environments:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
git clone https://github.com/EvolvingLMMs-Lab/lmms-eval
cd lmms-eval
uv sync  # Create/update the environment from uv.lock
uv run python -m lmms_eval --help  # Run commands with uv run
uv add <package>  # Add new dependencies (updates pyproject.toml & uv.lock)
```

### Alternative Installation

```bash
uv venv eval
uv venv --python 3.12
source eval/bin/activate
uv pip install git+https://github.com/EvolvingLMMs-Lab/lmms-eval.git
```

<details>
<summary>Reproducing LLaVA-1.5 Results</summary>

Instructions and resources are provided to reproduce the paper results of LLaVA-1.5, including an [environment install script](miscs/repr_scripts.sh) and [torch environment info](miscs/repr_torch_envs.txt).  See also the [results check](miscs/llava_result_check.md) for variations due to environment differences.
</details>

If testing caption datasets, `java==1.8.0` is needed for the pycocoeval API. Install with conda if needed:
```bash
conda install openjdk=8
```

Then verify your java version: `java -version`

<details>
<summary>Detailed Evaluation Results</summary>
<br>

Detailed dataset information and evaluation results of the LLaVA series are provided, including a [Google Sheet](https://docs.google.com/spreadsheets/d/1a5ImfdKATDI8T7Cwh6eH-bEsnQFzanFraFUgcS9KHWc/edit?usp=sharing) with live, updated results and raw data exported from Weights & Biases [here](https://docs.google.com/spreadsheets/d/1AvaEmuG4csSmXaHjgu4ei1KBMmNNW8wflOD_kkTDdv8/edit?usp=sharing).
</details>
<br>

If you wish to test [VILA](https://github.com/NVlabs/VILA), run the following install command:
```bash
pip install s2wrapper@git+https://github.com/bfshi/scaling_on_scales
```

We welcome feedback and contributions!  Please submit issues or pull requests on GitHub.

## Usage Examples

> Find more examples at [examples/models](examples/models)

**Evaluate OpenAI-Compatible Models**
```bash
bash examples/models/openai_compatible.sh
bash examples/models/xai_grok.sh
```

**Evaluate with vLLM**
```bash
bash examples/models/vllm_qwen2vl.sh
```

**Evaluate LLaVA-OneVision**
```bash
bash examples/models/llava_onevision.sh
```

**Evaluate LLaMA-3-Vision**
```bash
bash examples/models/llama_vision.sh
```

**Evaluate Qwen2-VL**
```bash
bash examples/models/qwen2_vl.sh
bash examples/models/qwen2_5_vl.sh
```

**Evaluate LLaVA on MME (requires cloning the LLaVA repo)**
```bash
bash examples/models/llava_next.sh
```

**Tensor Parallel Evaluation (llava-next-72b)**
```bash
bash examples/models/tensor_parallel.sh
```

**SGLang Evaluation (llava-next-72b)**
```bash
bash examples/models/sglang.sh
```

**vLLM Evaluation (llava-next-72b)**
```bash
bash examples/models/vllm_qwen2vl.sh
```

**Command Line Help**
```bash
python3 -m lmms_eval --help
```

**Environment Variables**
Ensure these environment variables are set before running experiments:

```bash
export OPENAI_API_KEY="<YOUR_API_KEY>"
export HF_HOME="<Path to HF cache>"
export HF_TOKEN="<YOUR_API_KEY>"
export HF_HUB_ENABLE_HF_TRANSFER="1"
export REKA_API_KEY="<YOUR_API_KEY>"
# Other variables include ANTHROPIC_API_KEY, DASHSCOPE_API_KEY, etc.
```

**Common Environment Issues - Troubleshooting**

Address potential issues with httpx, protobuf, and numpy by running the following:

```bash
python3 -m pip install httpx==0.23.3;
python3 -m pip install protobuf==3.20;
python3 -m pip install numpy==1.26; # or 1.26.x
python3 -m pip install sentencepiece;
```

## Adding Custom Models and Datasets

Refer to our detailed [documentation](docs/README.md) for instructions on extending LMMs-Eval.

## Acknowledgements

LMMs-Eval is based on the `lm-eval-harness` project.  For further insights, consult the [lm-eval-harness documentation](https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs).

---

**Key Differences from lm-eval-harness:**

*   Context building now only passes `idx` and processes image and doc during the model responding phase to avoid exploding CPU memory.
*   `Instance.args` includes a list of images for LMM input.
*   New model classes are required for LMMs due to differing input/output formats in Hugging Face, though efforts are ongoing to unify them.

---

## Citations

```shell
@misc{zhang2024lmmsevalrealitycheckevaluation,
      title={LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models},
      author={Kaichen Zhang and Bo Li and Peiyuan Zhang and Fanyi Pu and Joshua Adrian Cahyono and Kairui Hu and Shuai Liu and Yuanhan Zhang and Jingkang Yang and Chunyuan Li and Ziwei Liu},
      year={2024},
      eprint={2407.12772},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.12772},
}

@misc{lmms_eval2024,
    title={LMMs-Eval: Accelerating the Development of Large Multimoal Models},
    url={https://github.com/EvolvingLMMs-Lab/lmms-eval},
    author={Bo Li*, Peiyuan Zhang*, Kaichen Zhang*, Fanyi Pu*, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li and Ziwei Liu},
    publisher    = {Zenodo},
    version      = {v0.1.0},
    month={March},
    year={2024}
}