# StableAvatar: Generate Infinite-Length Audio-Driven Avatar Videos

Tired of short, low-quality avatar videos?  StableAvatar generates *infinite-length* and high-quality avatar videos from audio, preserving identity and realism without post-processing. Check out the original repository [here](https://github.com/Francis-Rings/StableAvatar).

<p align="center">
  <a href='https://francis-rings.github.io/StableAvatar'><img src='https://img.shields.io/badge/Project-Page-Green'></a>
  <a href='https://arxiv.org/abs/2508.08248'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>
  <a href='https://huggingface.co/FrancisRing/StableAvatar/tree/main'><img src='https://img.shields.io/badge/HuggingFace-Model-orange'></a>
  <a href='https://huggingface.co/spaces/YinmingHuang/StableAvatar'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a>
  <a href='https://www.youtube.com/watch?v=6lhvmbzvv3Y'><img src='https://img.shields.io/badge/YouTube-Watch-red?style=flat-square&logo=youtube'></a>
  <a href='https://www.bilibili.com/video/BV1hUt9z4EoQ'><img src='https://img.shields.io/badge/Bilibili-Watch-blue?style=flat-square&logo=bilibili'></a>
</p>

## Key Features

*   **Infinite-Length Video Generation:**  Create long, continuous avatar videos directly from audio.
*   **High Fidelity:**  Produce realistic and detailed videos.
*   **Identity Preservation:**  Maintain a consistent identity throughout the entire video.
*   **End-to-End Synthesis:**  No need for external face-swapping or restoration tools.
*   **Time-step-aware Audio Adapter:** Prevents error accumulation, enabling long-form video.
*   **Audio Native Guidance Mechanism:** Enhances audio synchronization.
*   **Dynamic Weighted Sliding-window Strategy:** Smooths transitions for seamless video.

## Demonstration Videos

<table border="0" style="width: 100%; text-align: left; margin-top: 20px;">
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/d7eca208-6a14-46af-b337-fb4d2b66ba8d" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/b15784b1-c013-4126-a764-10c844341a4e" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/87faa5c1-a118-4a03-a071-45f18e87e6a0" width="320" controls loop></video>
     </td>
  </tr>
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/531eb413-8993-4f8f-9804-e3c5ec5794d4" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/cdc603e2-df46-4cf8-a14e-1575053f996f" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/7022dc93-f705-46e5-b8fc-3a3fb755795c" width="320" controls loop></video>
     </td>
  </tr>
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/0ba059eb-ff6f-4d94-80e6-f758c613b737" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/03e6c1df-85c6-448d-b40d-aacb8add4e45" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/90b78154-dda0-4eaa-91fd-b5485b718a7f" width="320" controls loop></video>
     </td>
  </tr>
</table>

<p style="text-align: justify;">
  <span>Audio-driven avatar videos generated by StableAvatar, showcasing its ability to synthesize <b>infinite-length</b> and <b>identity-preserving videos</b>. All videos are <b>directly synthesized by StableAvatar without the use of any face-related post-processing tools</b>, such as face-swapping tools like FaceFusion or face restoration models like GFP-GAN and CodeFormer.</span>
</p>

<p align="center">
  <video src="https://github.com/user-attachments/assets/90691318-311e-40b9-9bd9-62db83ab1492" width="768" autoplay loop muted playsinline></video>
  <br/>
  <span>Comparison results between StableAvatar and state-of-the-art (SOTA) audio-driven avatar video generation models highlight the superior performance of StableAvatar in delivering <b>infinite-length, high-fidelity, identity-preserving avatar animation</b>.</span>
</p>

## Overview

<p align="center">
  <img src="assets/figures/framework.jpg" alt="model architecture" width="1280"/>
  </br>
  <i>The overview of the framework of StableAvatar.</i>
</p>

StableAvatar overcomes the limitations of existing audio-driven avatar video generation models by introducing innovative techniques that address error accumulation and enhance audio-visual synchronization. It utilizes a novel Time-step-aware Audio Adapter and an Audio Native Guidance Mechanism to ensure high-quality, infinite-length video output.

## News

*   `[2025-9-8]`: 🔥  New demo released!  Check it out on [YouTube](https://www.youtube.com/watch?v=GH4hrxIis3Q) and [Bilibili](https://www.bilibili.com/video/BV1jGYPzqEux).
*   `[2025-8-29]`: 🔥  Public demo now available on [Hugging Face Spaces](https://huggingface.co/spaces/YinmingHuang/StableAvatar) (Pro users only).
*   `[2025-8-18]`: 🔥  Runs on [ComfyUI](https://github.com/smthemex/ComfyUI_StableAvatar), 3x faster!  Thanks @[smthemex](https://github.com/smthemex)!
*   `[2025-8-16]`: 🔥  Finetuning and LoRA codes released!
*   `[2025-8-15]`: 🔥  Runs on Gradio Interface and [ComfyUI](https://github.com/smthemex/ComfyUI_StableAvatar). Thanks @[gluttony-10](https://space.bilibili.com/893892) and @[smthemex](https://github.com/smthemex)!
*   `[2025-8-13]`: 🔥  Support for new Blackwell series Nvidia chips.
*   `[2025-8-11]`: 🔥  Project page, code, technical report, and basic model checkpoint released.

## 🛠️ To-Do List

*   [x] StableAvatar-1.3B-basic
*   [x] Inference Code
*   [x] Data Pre-Processing Code (Audio Extraction)
*   [x] Data Pre-Processing Code (Vocal Separation)
*   [x] Training Code
*   [x] Full Finetuning Code
*   [x] Lora Training Code
*   [x] Lora Finetuning Code
*   [ ] Inference Code with Audio Native Guidance
*   [ ] StableAvatar-pro

## 🔑 Quickstart

Generate high-resolution, infinite-length videos using the basic model (Wan2.1-1.3B).  Adjust frame counts or resolution if you encounter memory issues.

### 🧱 Environment Setup

```bash
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu124
pip install -r requirements.txt
# Optional to install flash_attn to accelerate attention computation
pip install flash_attn
```

### 🧱 Environment setup for Blackwell series chips

```bash
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128
pip install -r requirements.txt
# Optional to install flash_attn to accelerate attention computation
pip install flash_attn
```

### 🧱 Download Weights

If you have trouble connecting to Hugging Face, set the environment variable: `export HF_ENDPOINT=https://hf-mirror.com`.

Download the model weights:

```bash
pip install "huggingface_hub[cli]"
cd StableAvatar
mkdir checkpoints
huggingface-cli download FrancisRing/StableAvatar --local-dir ./checkpoints
```

The file structure should be:

```
StableAvatar/
├── accelerate_config
├── deepspeed_config
├── examples
├── wan
├── checkpoints
│   ├── Kim_Vocal_2.onnx
│   ├── wav2vec2-base-960h
│   ├── Wan2.1-Fun-V1.1-1.3B-InP
│   └── StableAvatar-1.3B
├── inference.py
├── inference.sh
├── train_1B_square.py
├── train_1B_square.sh
├── train_1B_vec_rec.py
├── train_1B_vec_rec.sh
├── audio_extractor.py
├── vocal_seperator.py
├── requirement.txt 
```

### 🧱 Audio Extraction

Extract audio from a video:

```bash
python audio_extractor.py --video_path="path/test/video.mp4" --saved_audio_path="path/test/audio.wav"
```

### 🧱 Vocal Separation

Separate vocals from an audio file (optional, for improved lip sync):

```bash
pip install audio-separator[gpu]
python vocal_seperator.py --audio_separator_model_file="path/StableAvatar/checkpoints/Kim_Vocal_2.onnx" --audio_file_path="path/test/audio.wav" --saved_vocal_path="path/test/vocal.wav"
```

### 🧱 Base Model Inference

Run the inference script:

```bash
bash inference.sh
```

*   Modify `--width` and `--height` in `inference.sh` for resolution (512x512, 480x832, or 832x480).
*   `--output_dir` specifies the output directory.
*   `--validation_reference_path`, `--validation_driven_audio_path`, and `--validation_prompts` point to the reference image, audio, and text prompts.
*   Prompts are crucial: `[Description of first frame]-[Description of human behavior]-[Description of background (optional)]`.
*   Adjust `--sample_steps` (30-50 recommended) and `--overlap_window_length` (5-15 recommended).
*   Use `--sample_text_guide_scale` and `--sample_audio_guide_scale` (CFG scale, 3-6 recommended).

You can also run a Gradio interface:

```bash
python app.py
```

Examples are provided in `path/StableAvatar/examples`.

#### 💡 Tips

*   Choose the `transformer3d-square.pt` or `transformer3d-rec-vec.pt` model versions based on your video resolution.
*   Use `--GPU_memory_mode` in `inference.sh` to manage VRAM usage (e.g., `model_cpu_offload` for reduced memory).
*   Use multi-GPU inference by modifying `--ulysses_degree` and `--ring_degree` and consider `--fsdp_dit` for memory efficiency.
*  For a high-quality MP4 file with audio, use ffmpeg:

```bash
ffmpeg -i video_without_audio.mp4 -i /path/audio.wav -c:v copy -c:a aac -shortest /path/output_with_audio.mp4
```

### 🧱 Model Training

**🔥🔥This training guide is also valuable if you're training a conditioned Video Diffusion Transformer (DiT) model, such as Wan2.1.🔥🔥**

Organize your training data as follows:

```
talking_face_data/
├── rec
│   │  ├──speech
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  │  ├──frame_0.png
│   │  │  │  │  ├──frame_1.png
│   │  │  │  │  ├──frame_2.png
│   │  │  │  │  ├──...
│   │  │  │  ├──face_masks
│   │  │  │  │  ├──frame_0.png
│   │  │  │  │  ├──frame_1.png
│   │  │  │  │  ├──frame_2.png
│   │  │  │  │  ├──...
│   │  │  │  ├──lip_masks
│   │  │  │  │  ├──frame_0.png
│   │  │  │  │  ├──frame_1.png
│   │  │  │  │  ├──frame_2.png
│   │  │  │  │  ├──...
│   │  │  ├──00002
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──singing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──dancing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
├── vec
│   │  ├──speech
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──singing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──dancing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
├── square
│   │  ├──speech
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──singing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──dancing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
├── video_rec_path.txt
├── video_square_path.txt
└── video_vec_path.txt
```

*   `talking_face_data/square`, `talking_face_data/rec`, and `talking_face_data/vec` contain videos at 512x512, 480x832, and 832x480 resolutions.
*   Each video folder (e.g., `00001`) contains subfolders for different video types (speech, singing, and dancing).
*   `.png` images are named `frame_i.png`.
*   `images`, `face_masks`, and `lip_masks` store RGB frames, face masks, and lip masks.
*   `sub_clip.mp4` is the video and  `audio.wav` the audio.
*   `video_square_path.txt`, `video_rec_path.txt`, and `video_vec_path.txt` list video paths.

If you have raw videos, use `ffmpeg` to extract frames:

```bash
ffmpeg -i raw_video_1.mp4 -q:v 1 -start_number 0 path/StableAvatar/talking_face_data/rec/speech/00001/images/frame_%d.png
```

Extract face masks using the [StableAnimator repo](https://github.com/Francis-Rings/StableAnimator).

Extract lip masks:

```bash
pip install mediapipe
python lip_mask_extractor.py --folder_root="path/StableAvatar/talking_face_data/rec/singing" --start=1 --end=500
```

For audio extraction, refer to the Audio Extraction section.

Run the training scripts:

```bash
# Training StableAvatar on a single resolution setting (512x512) in a single machine
bash train_1B_square.sh
# Training StableAvatar on a single resolution setting (512x512) in multiple machines
bash train_1B_square_64.sh
# Training StableAvatar on a mixed resolution setting (480x832 and 832x480) in a single machine
bash train_1B_rec_vec.sh
# Training StableAvatar on a mixed resolution setting (480x832 and 832x480) in multiple machines
bash train_1B_rec_vec_64.sh
```

*   Modify training script parameters as needed.
*   `CUDA_VISIBLE_DEVICES` for GPU selection.
*   Set `--pretrained_model_name_or_path`, `--pretrained_wav2vec_path`, and `--output_dir`.
*   Use `--train_data_square_dir`, `--train_data_rec_dir`, and `--train_data_vec_dir` to specify your dataset paths.
*   `--validation_reference_path` and `--validation_driven_audio_path` are for validation.
*   Adjust `--video_sample_n_frames` and `--num_train_epochs` as required.

File structure:

```
StableAvatar/
├── accelerate_config
├── deepspeed_config
├── talking_face_data
├── examples
├── wan
├── checkpoints
│   ├── Kim_Vocal_2.onnx
│   ├── wav2vec2-base-960h
│   ├── Wan2.1-Fun-V1.1-1.3B-InP
│   └── StableAvatar-1.3B
├── inference.py
├── inference.sh
├── train_1B_square.py
├── train_1B_square.sh
├── train_1B_vec_rec.py
├── train_1B_vec_rec.sh
├── audio_extractor.py
├── vocal_seperator.py
├── requirement.txt 
```

**Important**: Training StableAvatar requires approximately 50GB of VRAM for mixed-resolution training, or 40GB if you train on 512x512 videos only.  The video backgrounds should be static and the audio should have minimal noise.

To train Wan2.1-14B-based StableAvatar:

```bash
# Training StableAvatar on a mixed resolution setting (480x832, 832x480, and 512x512) in multiple machines
huggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./checkpoints/Wan2.1-I2V-14B-480P
huggingface-cli download Wan-AI/Wan2.1-I2V-14B-720P --local-dir ./checkpoints/Wan2.1-I2V-14B-720P # Optional
bash train_14B.sh
```

We use deepspeed stage-2 for Wan2.1-14B training.

Lora training:

```bash
# Training StableAvatar-1.3B on a mixed resolution setting (480x832 and 832x480) in a single machine
bash train_1B_rec_vec_lora.sh
# Training StableAvatar-1.3B on a mixed resolution setting (480x832 and 832x480) in multiple machines
bash train_1B_rec_vec_lora_64.sh
# Lora-Training StableAvatar-14B on a mixed resolution setting (480x832, 832x480, and 512x512) in multiple machines
bash train_14B_lora.sh
```

Modify `--rank` and `--network_alpha` to control LoRA quality.

If you want to train 720P Wan2.1-1.3B-based or Wan2.1-14B-based StableAvatar, you can directly modify the height and width of the dataloader (480p-->720p) in `train_1B_square.py`/`train_1B_vec_rec.py`/`train_14B.py`.

### 🧱 Model Finetuning

Fully finetune StableAvatar:

```bash
# Finetuning StableAvatar on a mixed resolution setting (480x832 and 832x480) in a single machine
bash train_1B_rec_vec.sh --transformer_path="path/StableAvatar/checkpoints/StableAvatar-1.3B/transformer3d-square.pt"
# Finetuning StableAvatar on a mixed resolution setting (480x832 and 832x480) in multiple machines
bash train_1B_rec_vec_64.sh --transformer_path="path/StableAvatar/checkpoints/StableAvatar-1.3B/transformer3d-square.pt"
```
Lora finetuning StableAvatar:
```bash
# Lora-Finetuning StableAvatar-1.3B on a mixed resolution setting (480x832 and 832x480) in a single machine
bash train_1B_rec_vec_lora.sh --transformer_path="path/StableAvatar/checkpoints/StableAvatar-1.3B/transformer3d-square.pt"
```

Modify `--rank` and `--network_alpha` to control LoRA quality.

### 🧱 VRAM Requirement and Runtime

For a 5-second video (480x832, 25 fps), the basic model requires around 18GB VRAM and takes approximately 3 minutes on a 4090 GPU using `--GPU_memory_mode="model_full_load"`.

**🔥 While StableAvatar is designed to generate hours of video, the 3D VAE decoder's memory demands can be high.  You can run the VAE decoder on the CPU if needed. 🔥**

## Contact

For questions or suggestions:

Email: francisshuyuan@gmail.com

If you find this work useful, please consider giving a star ⭐ to the repository and citing it:

```bib
@article{tu2025stableavatar,
  title={StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation},
  author={Tu, Shuyuan and Pan, Yueming and Huang, Yinming and Han, Xintong and Xing, Zhen and Dai, Qi and Luo, Chong and Wu, Zuxuan and Jiang Yu-Gang},
  journal={arXiv preprint arXiv:2508.08248},
  year={2025}
}
```