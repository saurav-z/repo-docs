# StableAvatar: Generate Infinite-Length Audio-Driven Avatar Videos

StableAvatar revolutionizes audio-driven avatar video generation, enabling the creation of high-quality, infinite-length videos directly from audio inputs. Explore the project, code, and model here: [https://github.com/Francis-Rings/StableAvatar](https://github.com/Francis-Rings/StableAvatar).

[![Project Page](https://img.shields.io/badge/Project-Page-Green)](https://francis-rings.github.io/StableAvatar)
[![Paper](https://img.shields.io/badge/Paper-Arxiv-red)](https://arxiv.org/abs/2508.08248)
[![HuggingFace Model](https://img.shields.io/badge/HuggingFace-Model-orange)](https://huggingface.co/FrancisRing/StableAvatar/tree/main)
[![YouTube](https://img.shields.io/badge/YouTube-Watch-red?style=flat-square&logo=youtube)](https://www.youtube.com/watch?v=6lhvmbzvv3Y)
[![Bilibili](https://img.shields.io/badge/Bilibili-Watch-blue?style=flat-square&logo=bilibili)](https://www.bilibili.com/video/BV1hUt9z4EoQ)

**Key Features:**

*   **Infinite-Length Video Generation:**  Create videos of any length without quality degradation.
*   **ID-Preserving:**  Maintain consistent avatar identity throughout the video.
*   **Direct Synthesis:** No post-processing (face-swapping or restoration) is required.
*   **High Fidelity:**  Generate high-quality videos with excellent audio synchronization.
*   **End-to-End Solution:**  A complete video diffusion transformer for seamless avatar animation.

<table border="0" style="width: 100%; text-align: left; margin-top: 20px;">
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/d7eca208-6a14-46af-b337-fb4d2b66ba8d" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/b5902ac4-8188-4da8-b9e6-6df280690ed1" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/87faa5c1-a118-4a03-a071-45f18e87e6a0" width="320" controls loop></video>
     </td>
  </tr>
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/531eb413-8993-4f8f-9804-e3c5ec5794d4" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/cdc603e2-df46-4cf8-a14e-1575053f996f" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/7022dc93-f705-46e5-b8fc-3a3fb755795c" width="320" controls loop></video>
     </td>
  </tr>
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/0ba059eb-ff6f-4d94-80e6-f758c613b737" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/03e6c1df-85c6-448d-b40d-aacb8add4e45" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/90b78154-dda0-4eaa-91fd-b5485b718a7f" width="320" controls loop></video>
     </td>
  </tr>
</table>

<p style="text-align: justify;">
  <span>Audio-driven avatar videos generated by StableAvatar, showcasing its power to synthesize <b>infinite-length</b> and <b>ID-preserving videos</b>. All videos are <b>directly synthesized by StableAvatar without the use of any face-related post-processing tools</b>, such as the face-swapping tool FaceFusion or face restoration models like GFP-GAN and CodeFormer.</span>
</p>

<p align="center">
  <video src="https://github.com/user-attachments/assets/90691318-311e-40b9-9bd9-62db83ab1492" width="768" autoplay loop muted playsinline></video>
  <br/>
  <span>Comparison results between StableAvatar and state-of-the-art (SOTA) audio-driven avatar video generation models highlight the superior performance of StableAvatar in delivering <b>infinite-length, high-fidelity, identity-preserving avatar animation</b>.</span>
</p>


## Overview

<p align="center">
  <img src="assets/figures/framework.jpg" alt="model architecture" width="1280"/>
  </br>
  <i>The overview of the framework of StableAvatar.</i>
</p>

StableAvatar overcomes limitations of current audio-driven avatar video generation models by synthesizing long videos with natural audio synchronization and identity consistency. This is achieved through a novel architecture incorporating a Time-step-aware Audio Adapter and an Audio Native Guidance Mechanism.

## News

*   `[2025-8-16]`: 🔥 Release of finetuning and LoRA training/finetuning codes!
*   `[2025-8-15]`: 🔥 StableAvatar runs on Gradio interface and ComfyUI.
*   `[2025-8-13]`: 🔥 Support for new Blackwell series NVIDIA chips.
*   `[2025-8-11]`: 🔥 Project page, code, technical report, and basic model checkpoint released.

## 🛠️ To-Do List

*   \[x] StableAvatar-1.3B-basic
*   \[x] Inference Code
*   \[x] Data Pre-Processing Code (Audio Extraction)
*   \[x] Data Pre-Processing Code (Vocal Separation)
*   \[x] Training Code
*   \[x] Full Finetuning Code
*   \[x] LoRA Training Code
*   \[x] LoRA Finetuning Code
*   \[ ] Inference Code with Audio Native Guidance
*   \[ ] StableAvatar-pro

## 🔑 Quickstart

Generate infinite-length videos at 480x832, 832x480, or 512x512 resolution using the basic model (Wan2.1-1.3B-based).

### 🧱 Environment Setup

```bash
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu124
pip install -r requirements.txt
# Optional to install flash_attn to accelerate attention computation
pip install flash_attn
```

### 🧱 Environment setup for Blackwell series chips

```bash
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128
pip install -r requirements.txt
# Optional to install flash_attn to accelerate attention computation
pip install flash_attn
```

### 🧱 Download Weights

```bash
pip install "huggingface_hub[cli]"
cd StableAvatar
mkdir checkpoints
huggingface-cli download FrancisRing/StableAvatar --local-dir ./checkpoints
```

### 🧱 File Structure

```
StableAvatar/
├── accelerate_config
├── deepspeed_config
├── examples
├── wan
├── checkpoints
│   ├── Kim_Vocal_2.onnx
│   ├── wav2vec2-base-960h
│   ├── Wan2.1-Fun-V1.1-1.3B-InP
│   └── StableAvatar-1.3B
├── inference.py
├── inference.sh
├── train_1B_square.py
├── train_1B_square.sh
├── train_1B_vec_rec.py
├── train_1B_vec_rec.sh
├── audio_extractor.py
├── vocal_seperator.py
├── requirement.txt
```

### 🧱 Audio Extraction

```bash
python audio_extractor.py --video_path="path/test/video.mp4" --saved_audio_path="path/test/audio.wav"
```

### 🧱 Vocal Separation

```bash
pip install audio-separator[gpu]
python vocal_seperator.py --audio_separator_model_file="path/StableAvatar/checkpoints/Kim_Vocal_2.onnx" --audio_file_path="path/test/audio.wav" --saved_vocal_path="path/test/vocal.wav"
```

### 🧱 Base Model Inference

```bash
bash inference.sh
```

**Important Parameters for `inference.sh`:**

*   `--width` and `--height`:  Set output resolution (512x512, 480x832, 832x480).
*   `--output_dir`:  Specify output directory.
*   `--validation_reference_path`, `--validation_driven_audio_path`, `--validation_prompts`: Paths for the reference image, audio, and prompts.
*   `--pretrained_model_name_or_path`, `--pretrained_wav2vec_path`, `--transformer_path`: Paths to model weights.
*   `--sample_steps`:  Number of inference steps (recommended: 30-50).
*   `--overlap_window_length`:  Overlapping context length (recommended: 5-15).
*   `--clip_sample_n_frames`: Frames per context window.
*   `--sample_text_guide_scale`, `--sample_audio_guide_scale`: Guidance scale for text and audio (recommended: 3-6).

**Gradio Interface:**

```bash
python app.py
```

#### 💡Tips

*   Wan2.1-1.3B-based StableAvatar supports three resolutions: 512x512, 480x832, and 832x480.

*   Modify `--GPU_memory_mode` in `inference.sh` for GPU memory optimization:
    *   `model_full_load`:  Full model load.
    *   `sequential_cpu_offload`: Sequential CPU offload (approx. 3GB GPU memory, slower inference).
    *   `model_cpu_offload_and_qfloat8`:  CPU offload and quantization.
    *   `model_cpu_offload`:  CPU offload (reduces GPU memory usage significantly).

*   For multi-GPU inference, adjust `--ulysses_degree` and `--ring_degree` in `inference.sh`.

```bash
bash multiple_gpu_inference.sh
```

**Combine Audio and Video (FFmpeg):**

```bash
ffmpeg -i video_without_audio.mp4 -i /path/audio.wav -c:v copy -c:a aac -shortest /path/output_with_audio.mp4
```

### 🧱 Model Training

**Dataset Structure:**

```
talking_face_data/
├── rec
│   │  ├──speech
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  │  ├──frame_0.png
│   │  │  │  │  ├──frame_1.png
│   │  │  │  │  ├──frame_2.png
│   │  │  │  │  ├──...
│   │  │  │  ├──face_masks
│   │  │  │  │  ├──frame_0.png
│   │  │  │  │  ├──frame_1.png
│   │  │  │  │  ├──frame_2.png
│   │  │  │  │  ├──...
│   │  │  │  ├──lip_masks
│   │  │  │  │  ├──frame_0.png
│   │  │  │  │  ├──frame_1.png
│   │  │  │  │  ├──frame_2.png
│   │  │  │  │  ├──...
│   │  │  ├──00002
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──singing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──dancing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
├── vec
│   │  ├──speech
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──singing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──dancing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
├── square
│   │  ├──speech
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──singing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──dancing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
├── video_rec_path.txt
├── video_square_path.txt
└── video_vec_path.txt
```

*   `square`, `vec`, and `rec` folders contain videos at different resolutions.
*   Each video folder (e.g., `00001`) contains `sub_clip.mp4`, `audio.wav`, `images`, `face_masks`, and `lip_masks`.
*   `video_square_path.txt`, `video_rec_path.txt`, `video_vec_path.txt` list the video paths for each resolution.

**Training Commands:**

```bash
# Single GPU (512x512)
bash train_1B_square.sh
# Multi-GPU (512x512)
bash train_1B_square_64.sh
# Single GPU (mixed res)
bash train_1B_rec_vec.sh
# Multi-GPU (mixed res)
bash train_1B_rec_vec_64.sh

# Training Wan2.1-14B-based
huggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./checkpoints/Wan2.1-I2V-14B-480P
huggingface-cli download Wan-AI/Wan2.1-I2V-14B-720P --local-dir ./checkpoints/Wan2.1-I2V-14B-720P # Optional
bash train_14B.sh

# Lora Training
bash train_1B_rec_vec_lora.sh
bash train_1B_rec_vec_lora_64.sh
bash train_14B_lora.sh
```

**Key Training Parameters:**

*   `CUDA_VISIBLE_DEVICES`: GPU devices to use.
*   `--pretrained_model_name_or_path`:  Pretrained Wan2.1-1.3B/14B path.
*   `--pretrained_wav2vec_path`:  Pretrained Wav2Vec2.0 path.
*   `--output_dir`:  Checkpoint save path.
*   `--train_data_square_dir`, `--train_data_rec_dir`, `--train_data_vec_dir`: Paths to video list files.
*   `--validation_reference_path`, `--validation_driven_audio_path`:  Validation data paths.
*   `--video_sample_n_frames`: Frames per batch.
*   `--num_train_epochs`: Number of training epochs (set to infinite by default).
* `--rank` and `--network_alpha` control LoRA training quality.

**VRAM Requirements:**

*   Mixed-resolution training (480x832/832x480): ~50GB VRAM.
*   512x512 training: ~40GB VRAM.

**Important notes**:
*   The backgrounds of the training videos should remain static.
*   The audio should be clear and free from excessive background noise.

### 🧱 Model Finetuning

```bash
# Full finetuning
bash train_1B_rec_vec.sh  --transformer_path="path/StableAvatar/checkpoints/StableAvatar-1.3B/transformer3d-square.pt" #finetuning on a single machine
bash train_1B_rec_vec_64.sh  --transformer_path="path/StableAvatar/checkpoints/StableAvatar-1.3B/transformer3d-square.pt" #finetuning on multiple machines
# LoRA finetuning
bash train_1B_rec_vec_lora.sh --transformer_path="path/StableAvatar/checkpoints/StableAvatar-1.3B/transformer3d-square.pt"
```

### 🧱 VRAM and Runtime

For a 5-second video (480x832, 25 fps):

*   Basic model (model\_full\_load): ~18GB VRAM, 3 minutes on a 4090 GPU.

**StableAvatar is designed to generate hours of video with potentially minimal quality degradation.** The 3D VAE decoder can be run on the CPU if needed.

## Contact

For suggestions or inquiries, contact: francisshuyuan@gmail.com

If you find this work helpful, please consider giving this repository a star ⭐ and citing the paper:

```bib
@article{tu2025stableavatar,
  title={StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation},
  author={Tu, Shuyuan and Pan, Yueming and Huang, Yinming and Han, Xintong and Xing, Zhen and Dai, Qi and Luo, Chong and Wu, Zuxuan and Jiang Yu-Gang},
  journal={arXiv preprint arXiv:2508.08248},
  year={2025}
}
```