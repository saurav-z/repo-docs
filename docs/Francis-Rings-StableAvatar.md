# StableAvatar: Generate Infinite-Length Avatar Videos from Audio

**Create stunning, infinite-length avatar videos effortlessly with StableAvatar, a cutting-edge video diffusion transformer!** [[Original Repo](https://github.com/Francis-Rings/StableAvatar)]

[![Project Page](https://img.shields.io/badge/Project-Page-Green)](https://francis-rings.github.io/StableAvatar)
[![Paper](https://img.shields.io/badge/Paper-Arxiv-red)](https://arxiv.org/abs/2508.08248)
[![HuggingFace Model](https://img.shields.io/badge/HuggingFace-Model-orange)](https://huggingface.co/FrancisRing/StableAvatar/tree/main)
[![YouTube Video](https://img.shields.io/badge/YouTube-Watch-red?style=flat-square&logo=youtube)](https://www.youtube.com/watch?v=6lhvmbzvv3Y)
[![Bilibili Video](https://img.shields.io/badge/Bilibili-Watch-blue?style=flat-square&logo=bilibili)](https://www.bilibili.com/video/BV1hUt9z4EoQ)

StableAvatar is a groundbreaking approach to audio-driven avatar video generation, enabling the creation of videos with unparalleled length, fidelity, and identity preservation. It leverages a novel architecture that synthesizes high-quality, infinite-length videos directly, without the need for face-related post-processing techniques.

**Key Features:**

*   **Infinite-Length Video Generation:** Create avatar videos of any duration.
*   **High-Fidelity Results:** Produces videos with exceptional visual quality.
*   **Identity Preservation:** Maintains consistent facial features throughout the video.
*   **End-to-End Solution:** No need for external face-swapping or restoration tools.
*   **Time-step-aware Audio Adapter:** Prevents error accumulation and maintains audio-visual synchronization.
*   **Audio Native Guidance Mechanism:** Improves audio synchronization with a dynamic guidance signal.
*   **Dynamic Weighted Sliding-window Strategy:** Ensures smoothness in long videos.

<table border="0" style="width: 100%; text-align: left; margin-top: 20px;">
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/d7eca208-6a14-46af-b337-fb4d2b66ba8d" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/b5902ac4-8188-4da8-b9e6-6df280690ed1" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/87faa5c1-a118-4a03-a071-45f18e87e6a0" width="320" controls loop></video>
     </td>
  </tr>
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/531eb413-8993-4f8f-9804-e3c5ec5794d4" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/cdc603e2-df46-4cf8-a14e-1575053f996f" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/7022dc93-f705-46e5-b8fc-3a3fb755795c" width="320" controls loop></video>
     </td>
  </tr>
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/0ba059eb-ff6f-4d94-80e6-f758c613b737" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/03e6c1df-85c6-448d-b40d-aacb8add4e45" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/90b78154-dda0-4eaa-91fd-b5485b718a7f" width="320" controls loop></video>
     </td>
  </tr>
</table>

<p style="text-align: justify;">
  <span>Audio-driven avatar videos generated by StableAvatar, showing its power to synthesize <b>infinite-length</b> and <b>ID-preserving videos</b>. All videos are <b>directly synthesized by StableAvatar without the use of any face-related post-processing tools</b>, such as the face-swapping tool FaceFusion or face restoration models like GFP-GAN and CodeFormer.</span>
</p>

<p align="center">
  <video src="https://github.com/user-attachments/assets/90691318-311e-40b9-9bd9-62db83ab1492" width="768" autoplay loop muted playsinline></video>
  <br/>
  <span>Comparison results between StableAvatar and state-of-the-art (SOTA) audio-driven avatar video generation models highlight the superior performance of StableAvatar in delivering <b>infinite-length, high-fidelity, identity-preserving avatar animation</b>.</span>
</p>

## Overview

<p align="center">
  <img src="assets/figures/framework.jpg" alt="model architecture" width="1280"/>
  </br>
  <i>The overview of the framework of StableAvatar.</i>
</p>

StableAvatar overcomes the limitations of existing models by introducing innovative solutions like the Time-step-aware Audio Adapter and the Audio Native Guidance Mechanism, enabling the generation of long, high-quality videos.

## What's New

*   `[2025-8-18]`:üî• StableAvatar can run on [ComfyUI](https://github.com/smthemex/ComfyUI_StableAvatar) in just 10 steps, making it 3x faster. Thanks @[smthemex](https://github.com/smthemex) for the contribution.
*   `[2025-8-16]`:üî• We release the finetuning codes and lora training/finetuning codes! Other codes will be public as soon as possible. Stay tuned!
*   `[2025-8-15]`:üî• StableAvatar can run on Gradio Interface. Thanks @[gluttony-10](https://space.bilibili.com/893892) for the contribution!
*   `[2025-8-15]`:üî• StableAvatar can run on [ComfyUI](https://github.com/smthemex/ComfyUI_StableAvatar). Thanks @[smthemex](https://github.com/smthemex) for the contribution.
*   `[2025-8-13]`:üî• Added changes to run StableAvatar on the new Blackwell series Nvidia chips, including the RTX 6000 Pro.
*   `[2025-8-11]`:üî• The project page, code, technical report and [a basic model checkpoint](https://huggingface.co/FrancisRing/StableAvatar/tree/main) are released. Further lora training codes, the evaluation dataset and StableAvatar-pro will be released very soon. Stay tuned!

## To-Do List

*   [x] StableAvatar-1.3B-basic
*   [x] Inference Code
*   [x] Data Pre-Processing Code (Audio Extraction)
*   [x] Data Pre-Processing Code (Vocal Separation)
*   [x] Training Code
*   [x] Full Finetuning Code
*   [x] Lora Training Code
*   [x] Lora Finetuning Code
*   [ ] Inference Code with Audio Native Guidance
*   [ ] StableAvatar-pro

## Quickstart Guide

The basic version of StableAvatar (Wan2.1-1.3B-based) supports generating infinite-length videos at 480x832, 832x480, or 512x512 resolutions.  Adjust animation frame count or resolution to manage memory issues.

### Environment Setup

```bash
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu124
pip install -r requirements.txt
# Optional to install flash_attn to accelerate attention computation
pip install flash_attn
```

### Environment Setup for Blackwell series chips

```bash
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128
pip install -r requirements.txt
# Optional to install flash_attn to accelerate attention computation
pip install flash_attn
```

### Download Weights

Download the model weights from Hugging Face:

```bash
pip install "huggingface_hub[cli]"
cd StableAvatar
mkdir checkpoints
huggingface-cli download FrancisRing/StableAvatar --local-dir ./checkpoints
```

Ensure the following file structure:

```
StableAvatar/
‚îú‚îÄ‚îÄ accelerate_config
‚îú‚îÄ‚îÄ deepspeed_config
‚îú‚îÄ‚îÄ examples
‚îú‚îÄ‚îÄ wan
‚îú‚îÄ‚îÄ checkpoints
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Kim_Vocal_2.onnx
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ wav2vec2-base-960h
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Wan2.1-Fun-V1.1-1.3B-InP
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ StableAvatar-1.3B
‚îú‚îÄ‚îÄ inference.py
‚îú‚îÄ‚îÄ inference.sh
‚îú‚îÄ‚îÄ train_1B_square.py
‚îú‚îÄ‚îÄ train_1B_square.sh
‚îú‚îÄ‚îÄ train_1B_vec_rec.py
‚îú‚îÄ‚îÄ train_1B_vec_rec.sh
‚îú‚îÄ‚îÄ audio_extractor.py
‚îú‚îÄ‚îÄ vocal_seperator.py
‚îú‚îÄ‚îÄ requirement.txt 
```

### Audio Extraction

Extract audio from a video file:

```bash
python audio_extractor.py --video_path="path/test/video.mp4" --saved_audio_path="path/test/audio.wav"
```

### Vocal Separation

Separate vocals from audio for better lip-sync:

```bash
pip install audio-separator[gpu]
python vocal_seperator.py --audio_separator_model_file="path/StableAvatar/checkpoints/Kim_Vocal_2.onnx" --audio_file_path="path/test/audio.wav" --saved_vocal_path="path/test/vocal.wav"
```

### Base Model Inference

Run inference using `inference.sh`.  Customize settings for desired resolution, prompts, and model paths.

```bash
bash inference.sh
```

**Key Parameters to Customize:**

*   `--width` and `--height`:  Set the output video resolution (512x512, 480x832, or 832x480).
*   `--output_dir`:  Specify the save location for generated videos.
*   `--validation_reference_path`, `--validation_driven_audio_path`, `--validation_prompts`: Paths for reference image, audio, and text prompts.
*   `--pretrained_model_name_or_path`, `--pretrained_wav2vec_path`, `--transformer_path`:  Paths to pre-trained model weights.
*   `--sample_steps`, `--overlap_window_length`, `--clip_sample_n_frames`: Inference parameters (recommended: sample_steps: 30-50, overlap_window_length: 5-15).
*   `--sample_text_guide_scale`, `--sample_audio_guide_scale`:  CFG scales (recommended: 3-6). Increase audio cfg for lip-sync improvements.
*   `--GPU_memory_mode`: `model_full_load`, `sequential_cpu_offload`, `model_cpu_offload_and_qfloat8`, and `model_cpu_offload`

**Important Notes:**

*   Wan2.1-1.3B supports 512x512, 480x832, and 832x480 resolutions.
*   Use `--GPU_memory_mode` to manage GPU memory constraints. `sequential_cpu_offload` (approx 3GB) and `model_cpu_offload` (reduce usage by approx half) are helpful.
*   For multi-GPU inference, adjust `--ulysses_degree` and `--ring_degree` in `inference.sh`.  (Ensure  `ulysses_degree * ring_degree = total GPUs / world-size`)
*   If you have multiple Gpus, you can run Multi-GPU inference to speed up by modifying "--ulysses_degree" and "--ring_degree" in `inference.sh`. For example, if you have 8 GPUs, you can set `--ulysses_degree=4` and `--ring_degree=2`. Notably, you have to ensure ulysses_degree*ring_degree=total GPU number/world-size. Moreover, you can also add `--fsdp_dit` in `inference.sh` to activate FSDP in DiT to further reduce GPU memory consumption.

  *   You can fun the following command:
```
bash multiple_gpu_inference.sh
```

*   Use `ffmpeg` to add audio to your video output:

```bash
ffmpeg -i video_without_audio.mp4 -i /path/audio.wav -c:v copy -c:a aac -shortest /path/output_with_audio.mp4
```
### Gradio Interface

Launch a Gradio interface for easier use:

```bash
python app.py
```

## Model Training

Detailed instructions for training StableAvatar, including dataset formatting, are included in the original README.

**Key aspects of training are highlighted below:**

*   **Dataset Organization:**
    The training data requires specific file organization as outlined in the original README, including subfolders for different video types and individual video information.
*   **Data Preparation:** Instructions for extracting and preparing frames, face masks, lip masks, and audio are provided.
*   **Training Scripts:** Run the `train_1B_square.sh` (single resolution, single machine), `train_1B_square_64.sh` (single resolution, multiple machines), `train_1B_rec_vec.sh` (mixed resolution, single machine), or `train_1B_rec_vec_64.sh` (mixed resolution, multiple machines) to train the model. The training setup consists of 8 nodes, each equipped with 8 NVIDIA A100 80GB GPUs, for training StableAvatar.
*   **VRAM Requirements:**  Training requires approx. 50GB VRAM (mixed resolution) or 40GB (512x512 only).
*   **Static Backgrounds:** The video backgrounds should remain static for accurate reconstruction loss calculation.
*   **Clear Audio:** Ensure the audio is clear with minimal background noise.

###  Finetuning & Lora Training

Finetuning and Lora training is also available and detailed in the original README.

### VRAM Requirement and Runtime
For the 5s video (480x832, fps=25), the basic model (--GPU_memory_mode="model_full_load") requires approximately 18GB VRAM and finishes in 3 minutes on a 4090 GPU.
Theoretically, StableAvatar is capable of synthesizing hours of video without significant quality degradation; however, the 3D VAE decoder demands significant GPU memory, especially when decoding 10k+ frames. You have the option to run the VAE decoder on CPU.

## Contact

For questions or suggestions, please contact:

*   Email: francisshuyuan@gmail.com

**If you find this project valuable, please consider giving it a star ‚≠ê and citing it ‚ù§Ô∏è:**

```bib
@article{tu2025stableavatar,
  title={StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation},
  author={Tu, Shuyuan and Pan, Yueming and Huang, Yinming and Han, Xintong and Xing, Zhen and Dai, Qi and Luo, Chong and Wu, Zuxuan and Jiang Yu-Gang},
  journal={arXiv preprint arXiv:2508.08248},
  year={2025}
}
```