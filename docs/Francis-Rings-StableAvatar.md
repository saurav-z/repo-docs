# StableAvatar: Generate Infinite-Length Avatar Videos from Audio 

**Unleash the power of audio to create endless, high-quality avatar videos that preserve identity with StableAvatar!** ([Original Repo](https://github.com/Francis-Rings/StableAvatar))

[![Project Page](https://img.shields.io/badge/Project-Page-Green)](https://francis-rings.github.io/StableAvatar)
[![Arxiv](https://img.shields.io/badge/Paper-Arxiv-red)](https://arxiv.org/abs/2508.08248)
[![HuggingFace Model](https://img.shields.io/badge/HuggingFace-Model-orange)](https://huggingface.co/FrancisRing/StableAvatar/tree/main)
[![YouTube Demo](https://img.shields.io/badge/YouTube-Watch-red?style=flat-square&logo=youtube)](https://www.youtube.com/watch?v=6lhvmbzvv3Y)
[![Bilibili Demo](https://img.shields.io/badge/Bilibili-Watch-blue?style=flat-square&logo=bilibili)](https://www.bilibili.com/video/BV1hUt9z4EoQ)

StableAvatar revolutionizes audio-driven avatar video generation, enabling the synthesis of infinite-length, high-fidelity videos that maintain the subject's identity. Leveraging a novel end-to-end video diffusion transformer, StableAvatar bypasses the need for post-processing tools, delivering impressive results directly from audio and a reference image.

**Key Features:**

*   **Infinite-Length Video Generation:** Generate avatar videos of unlimited duration.
*   **Identity Preservation:**  Guaranteed identity consistency throughout the video.
*   **High-Fidelity Output:**  Produces high-quality videos directly without face-related post-processing.
*   **Audio-Driven Animation:**  Synchronizes avatar movements perfectly with the provided audio.
*   **End-to-End Solution:** No external face-swapping or restoration tools required.
*   **Supports Multiple Resolutions:** 512x512, 480x832, and 832x480.

<table border="0" style="width: 100%; text-align: left; margin-top: 20px;">
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/d7eca208-6a14-46af-b337-fb4d2b66ba8d" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/b5902ac4-8188-4da8-b9e6-6df280690ed1" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/87faa5c1-a118-4a03-a071-45f18e87e6a0" width="320" controls loop></video>
     </td>
  </tr>
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/531eb413-8993-4f8f-9804-e3c5ec5794d4" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/cdc603e2-df46-4cf8-a14e-1575053f996f" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/7022dc93-f705-46e5-b8fc-3a3fb755795c" width="320" controls loop></video>
     </td>
  </tr>
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/0ba059eb-ff6f-4d94-80e6-f758c613b737" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/03e6c1df-85c6-448d-b40d-aacb8add4e45" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/90b78154-dda0-4eaa-91fd-b5485b718a7f" width="320" controls loop></video>
     </td>
  </tr>
</table>

<p style="text-align: justify;">
  <span>Audio-driven avatar videos generated by StableAvatar, showing its power to synthesize <b>infinite-length</b> and <b>ID-preserving videos</b>. All videos are <b>directly synthesized by StableAvatar without the use of any face-related post-processing tools</b>, such as the face-swapping tool FaceFusion or face restoration models like GFP-GAN and CodeFormer.</span>
</p>

<p align="center">
  <video src="https://github.com/user-attachments/assets/90691318-311e-40b9-9bd9-62db83ab1492" width="768" autoplay loop muted playsinline></video>
  <br/>
  <span>Comparison results between StableAvatar and state-of-the-art (SOTA) audio-driven avatar video generation models highlight the superior performance of StableAvatar in delivering <b>infinite-length, high-fidelity, identity-preserving avatar animation</b>.</span>
</p>

## Overview

<p align="center">
  <img src="assets/figures/framework.jpg" alt="model architecture" width="1280"/>
  </br>
  <i>The overview of the framework of StableAvatar.</i>
</p>

StableAvatar overcomes limitations in existing models by integrating a novel Time-step-aware Audio Adapter and Audio Native Guidance Mechanism for improved audio synchronization and smoothness. This innovative approach allows StableAvatar to generate long, natural-looking videos without error accumulation.

## News

*   `[2025-08-18]`: 🔥 StableAvatar now runs on [ComfyUI](https://github.com/smthemex/ComfyUI_StableAvatar), making it 3x faster! Thanks @[smthemex](https://github.com/smthemex)!
*   `[2025-08-16]`: 🔥 Finetuning and LoRA training codes are released!
*   `[2025-08-15]`: 🔥 Gradio Interface available! Thanks @[gluttony-10](https://space.bilibili.com/893892)!
*   `[2025-08-15]`: 🔥  Compatible with [ComfyUI](https://github.com/smthemex/ComfyUI_StableAvatar). Thanks @[smthemex](https://github.com/smthemex)!
*   `[2025-08-13]`: 🔥  Updated to support the new Blackwell series Nvidia chips.
*   `[2025-08-11]`: 🔥  Project page, code, technical report, and basic model checkpoint released.

## 🛠️ To-Do List

*   [x] StableAvatar-1.3B-basic
*   [x] Inference Code
*   [x] Data Pre-Processing Code (Audio Extraction)
*   [x] Data Pre-Processing Code (Vocal Separation)
*   [x] Training Code
*   [x] Full Finetuning Code
*   [x] Lora Training Code
*   [x] Lora Finetuning Code
*   [ ] Inference Code with Audio Native Guidance
*   [ ] StableAvatar-pro

## 🚀 Quickstart

The basic version of the model supports generating **infinite-length videos at 480x832, 832x480, or 512x512 resolution**. Adjust frame numbers or output resolution if you encounter memory issues.

### 🧱 Environment Setup

```bash
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu124
pip install -r requirements.txt
# Optional to install flash_attn to accelerate attention computation
pip install flash_attn
```

### 🧱 Environment Setup for Blackwell Series Chips

```bash
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128
pip install -r requirements.txt
# Optional to install flash_attn to accelerate attention computation
pip install flash_attn
```

### 🧱 Download Weights

If you have trouble with Hugging Face connections, use the mirror endpoint: `export HF_ENDPOINT=https://hf-mirror.com`.

```bash
pip install "huggingface_hub[cli]"
cd StableAvatar
mkdir checkpoints
huggingface-cli download FrancisRing/StableAvatar --local-dir ./checkpoints
```

Organize the weights as follows:

```
StableAvatar/
├── accelerate_config
├── deepspeed_config
├── examples
├── wan
├── checkpoints
│   ├── Kim_Vocal_2.onnx
│   ├── wav2vec2-base-960h
│   ├── Wan2.1-Fun-V1.1-1.3B-InP
│   └── StableAvatar-1.3B
├── inference.py
├── inference.sh
├── train_1B_square.py
├── train_1B_square.sh
├── train_1B_vec_rec.py
├── train_1B_vec_rec.sh
├── audio_extractor.py
├── vocal_seperator.py
├── requirement.txt
```

### 🧱 Audio Extraction

Extract audio from video files:

```bash
python audio_extractor.py --video_path="path/test/video.mp4" --saved_audio_path="path/test/audio.wav"
```

### 🧱 Vocal Separation

Separate vocals from audio for improved lip-sync:

```bash
pip install audio-separator[gpu]
python vocal_seperator.py --audio_separator_model_file="path/StableAvatar/checkpoints/Kim_Vocal_2.onnx" --audio_file_path="path/test/audio.wav" --saved_vocal_path="path/test/vocal.wav"
```

### 🧱 Base Model Inference

Run the example `inference.sh` script, or modify it for your needs:

```bash
bash inference.sh
```

The Wan2.1-1.3B-based model supports generating audio-driven avatar videos at 512x512, 480x832, and 832x480 resolutions. Configure the resolution using `--width` and `--height` in `inference.sh`. Set the output path with `--output_dir`.  Use `--validation_reference_path`, `--validation_driven_audio_path`, and `--validation_prompts` to specify the reference image, audio, and text prompts.

Prompts are critical. Recommended format: `[Description of first frame]-[Description of human behavior]-[Description of background (optional)]`.

Specify the model paths with `--pretrained_model_name_or_path`, `--pretrained_wav2vec_path`, and `--transformer_path`.

Adjust `--sample_steps`, `--overlap_window_length`, and `--clip_sample_n_frames` for quality and speed.  Text and audio CFG scales (`--sample_text_guide_scale` and `--sample_audio_guide_scale`) are also important.

You can also launch a Gradio interface:

```bash
python app.py
```

See the `/examples` directory for 6 example cases.

#### 💡 Tips

*   Use the `transformer3d-square.pt` or `transformer3d-rec-vec.pt` weights to switch between training datasets.
*   If GPU memory is limited, use `--GPU_memory_mode` (e.g., `sequential_cpu_offload`, `model_cpu_offload`, or `model_cpu_offload_and_qfloat8`).
*   Use multi-GPU inference with `--ulysses_degree` and `--ring_degree`.
*   Use FSDP in DiT with `--fsdp_dit` for further memory reduction.

To create a high-quality MP4 with audio, use `ffmpeg` on the output:

```bash
ffmpeg -i video_without_audio.mp4 -i /path/audio.wav -c:v copy -c:a aac -shortest /path/output_with_audio.mp4
```

### 🧱 Model Training

**🔥🔥This tutorial is also applicable to training conditioned Video Diffusion Transformer (DiT) models, like Wan2.1!🔥🔥**

Organize your training data as specified in the README.

```
talking_face_data/
├── rec
│   │  ├──speech
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  │  ├──frame_0.png
│   │  │  │  │  ├──frame_1.png
│   │  │  │  │  ├──frame_2.png
│   │  │  │  │  ├──...
│   │  │  │  ├──face_masks
│   │  │  │  │  ├──frame_0.png
│   │  │  │  │  ├──frame_1.png
│   │  │  │  │  ├──frame_2.png
│   │  │  │  │  ├──...
│   │  │  │  ├──lip_masks
│   │  │  │  │  ├──frame_0.png
│   │  │  │  │  ├──frame_1.png
│   │  │  │  │  ├──frame_2.png
│   │  │  │  │  ├──...
│   │  │  ├──00002
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──singing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──dancing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
├── vec
│   │  ├──speech
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──singing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──dancing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
├── square
│   │  ├──speech
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──singing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
│   │  ├──dancing
│   │  │  ├──00001
│   │  │  │  ├──sub_clip.mp4
│   │  │  │  ├──audio.wav
│   │  │  │  ├──images
│   │  │  │  ├──face_masks
│   │  │  │  ├──lip_masks
│   │  │  └──...
├── video_rec_path.txt
├── video_square_path.txt
└── video_vec_path.txt
```

Prepare your video data and extract frames with ffmpeg:

```bash
ffmpeg -i raw_video_1.mp4 -q:v 1 -start_number 0 path/StableAvatar/talking_face_data/rec/speech/00001/images/frame_%d.png
```

Extract face and lip masks. For face masks, see [StableAnimator repo](https://github.com/Francis-Rings/StableAnimator). For lip masks:

```bash
pip install mediapipe
python lip_mask_extractor.py --folder_root="path/StableAvatar/talking_face_data/rec/singing" --start=1 --end=500
```

Then run the training scripts:

```bash
# Train on a single resolution (512x512) on a single machine
bash train_1B_square.sh
# Train on a single resolution (512x512) on multiple machines
bash train_1B_square_64.sh
# Train on mixed resolutions (480x832, 832x480) on a single machine
bash train_1B_rec_vec.sh
# Train on mixed resolutions (480x832, 832x480) on multiple machines
bash train_1B_rec_vec_64.sh
```

Configure the training parameters as detailed in the training scripts.

Training requires considerable VRAM, depending on resolution and training setup. The VRAM requirement is approximately 40GB for training on 512x512 videos.

The backgrounds of training videos should be static, and the audio should be clear.

For training Wan2.1-14B:

```bash
# Train on mixed resolutions (480x832, 832x480, and 512x512) on multiple machines
huggingface-cli download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./checkpoints/Wan2.1-I2V-14B-480P
huggingface-cli download Wan-AI/Wan2.1-I2V-14B-720P --local-dir ./checkpoints/Wan2.1-I2V-14B-720P # Optional
bash train_14B.sh
```

### 🧱 Model Finetuning and LoRA Training

Finetuning StableAvatar: add `--transformer_path` to the `train_1B_rec_vec.sh` or `train_1B_rec_vec_64.sh`:

```bash
# Finetune on mixed resolutions (480x832, 832x480) on a single machine
bash train_1B_rec_vec.sh
# Finetune on mixed resolutions (480x832, 832x480) on multiple machines
bash train_1B_rec_vec_64.sh
```

LoRA Training:

```bash
# LoRA Training
bash train_1B_rec_vec_lora.sh
```

Modify `--rank` and `--network_alpha` for LoRA quality control.

### 🧱 VRAM and Runtime

The basic model requires ~18GB of VRAM and finishes in ~3 minutes for a 5-second video (480x832, fps=25) on a 4090 GPU.
Theoretically, StableAvatar can generate hours of video, but the 3D VAE decoder demands significant GPU memory. You can run the VAE decoder on CPU to alleviate this problem.

## Contact

For any suggestions, or if you find our work helpful, please reach out:

Email: francisshuyuan@gmail.com

If you find our work useful, **please consider giving a star ⭐ to this repository and citing it ❤️**:

```bib
@article{tu2025stableavatar,
  title={StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation},
  author={Tu, Shuyuan and Pan, Yueming and Huang, Yinming and Han, Xintong and Xing, Zhen and Dai, Qi and Luo, Chong and Wu, Zuxuan and Jiang Yu-Gang},
  journal={arXiv preprint arXiv:2508.08248},
  year={2025}
}
```