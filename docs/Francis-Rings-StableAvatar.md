# StableAvatar: Generate Infinite-Length Audio-Driven Avatar Videos

**Create stunning, endlessly long avatar videos from audio with StableAvatar, the cutting-edge AI model!** ([Original Repo](https://github.com/Francis-Rings/StableAvatar))

<a href='https://francis-rings.github.io/StableAvatar'><img src='https://img.shields.io/badge/Project-Page-Green'></a> <a href='https://arxiv.org/abs/2508.08248'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a> <a href='https://huggingface.co/FrancisRing/StableAvatar/tree/main'><img src='https://img.shields.io/badge/HuggingFace-Model-orange'></a> <a href='https://huggingface.co/spaces/YinmingHuang/StableAvatar'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a> <a href='https://www.youtube.com/watch?v=6lhvmbzvv3Y'><img src='https://img.shields.io/badge/YouTube-Watch-red?style=flat-square&logo=youtube'></a> <a href='https://www.bilibili.com/video/BV1hUt9z4EoQ'><img src='https://img.shields.io/badge/Bilibili-Watch-blue?style=flat-square&logo=bilibili'></a>

**Key Features:**

*   **Infinite-Length Video Generation:** Produce avatar videos of virtually unlimited length.
*   **Identity Preservation:** Maintain consistent identity throughout the entire video.
*   **High-Fidelity Results:** Generate high-quality avatar animations with impressive detail.
*   **End-to-End Solution:** StableAvatar synthesizes videos directly without the need for face-swapping or restoration tools.
*   **Audio Synchronization:** Enjoy natural and accurate audio-visual synchronization.
*   **Flexible Resolutions:** Supports generating videos at 512x512, 480x832, and 832x480 resolutions.

<table border="0" style="width: 100%; text-align: left; margin-top: 20px;">
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/d7eca208-6a14-46af-b337-fb4d2b66ba8d" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/b5902ac4-8188-4da8-b9e6-6df280690ed1" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/87faa5c1-a118-4a03-a071-45f18e87e6a0" width="320" controls loop></video>
     </td>
  </tr>
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/531eb413-8993-4f8f-9804-e3c5ec5794d4" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/cdc603e2-df46-4cf8-a14e-1575053f996f" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/7022dc93-f705-46e5-b8fc-3a3fb755795c" width="320" controls loop></video>
     </td>
  </tr>
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/0ba059eb-ff6f-4d94-80e6-f758c613b737" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/03e6c1df-85c6-448d-b40d-aacb8add4e45" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/90b78154-dda0-4eaa-91fd-b5485b718a7f" width="320" controls loop></video>
     </td>
  </tr>
</table>

<p style="text-align: justify;">
  <span>Audio-driven avatar videos generated by StableAvatar, showcasing its ability to synthesize <b>infinite-length</b> and <b>ID-preserving videos</b>. All videos are <b>directly synthesized by StableAvatar without the use of any face-related post-processing tools</b>, such as the face-swapping tool FaceFusion or face restoration models like GFP-GAN and CodeFormer.</span>
</p>

<p align="center">
  <video src="https://github.com/user-attachments/assets/90691318-311e-40b9-9bd9-62db83ab1492" width="768" autoplay loop muted playsinline></video>
  <br/>
  <span>Comparison results between StableAvatar and state-of-the-art (SOTA) audio-driven avatar video generation models highlight the superior performance of StableAvatar in delivering <b>infinite-length, high-fidelity, identity-preserving avatar animation</b>.</span>
</p>

## Overview

<p align="center">
  <img src="assets/figures/framework.jpg" alt="model architecture" width="1280"/>
  </br>
  <i>The overview of the framework of StableAvatar.</i>
</p>

StableAvatar is a groundbreaking end-to-end video diffusion transformer that overcomes limitations in existing models to generate long, high-quality avatar videos with seamless audio synchronization and identity consistency. Key innovations include a Time-step-aware Audio Adapter to prevent error accumulation, and an Audio Native Guidance Mechanism to enhance audio synchronization. These advancements enable StableAvatar to produce unparalleled results without the need for post-processing.

## News

*   `[2025-8-29]`: 🔥 Live demo on [Hugging Face Spaces](https://huggingface.co/spaces/YinmingHuang/StableAvatar) (for Hugging Face Pro users).
*   `[2025-8-18]`: 🔥 Run StableAvatar on [ComfyUI](https://github.com/smthemex/ComfyUI_StableAvatar) with just 10 steps.
*   `[2025-8-16]`: 🔥 Finetuning and LoRA training codes released.
*   `[2025-8-15]`: 🔥 Gradio Interface support and ComfyUI integration.
*   `[2025-8-13]`: 🔥 Support for new Blackwell series Nvidia chips.
*   `[2025-8-11]`: 🔥 Project page, code, technical report, and a basic model checkpoint released.

## 🛠️ To-Do List

*   \[x] StableAvatar-1.3B-basic
*   \[x] Inference Code
*   \[x] Data Pre-Processing Code (Audio Extraction)
*   \[x] Data Pre-Processing Code (Vocal Separation)
*   \[x] Training Code
*   \[x] Full Finetuning Code
*   \[x] Lora Training Code
*   \[x] Lora Finetuning Code
*   \[ ] Inference Code with Audio Native Guidance
*   \[ ] StableAvatar-pro

## 🔑 Quickstart

The basic version of the model checkpoint (Wan2.1-1.3B-based) supports generating infinite-length videos at 480x832, 832x480, or 512x512 resolutions.

### 🧱 Environment Setup

```bash
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu124
pip install -r requirements.txt
# Optional to install flash_attn to accelerate attention computation
pip install flash_attn
```

### 🧱 Environment setup for Blackwell series chips

```bash
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128
pip install -r requirements.txt
# Optional to install flash_attn to accelerate attention computation
pip install flash_attn
```

### 🧱 Download Weights

```bash
pip install "huggingface_hub[cli]"
cd StableAvatar
mkdir checkpoints
huggingface-cli download FrancisRing/StableAvatar --local-dir ./checkpoints
```

Organize the weights as follows:

```
StableAvatar/
├── accelerate_config
├── deepspeed_config
├── examples
├── wan
├── checkpoints
│   ├── Kim_Vocal_2.onnx
│   ├── wav2vec2-base-960h
│   ├── Wan2.1-Fun-V1.1-1.3B-InP
│   └── StableAvatar-1.3B
├── inference.py
├── inference.sh
├── train_1B_square.py
├── train_1B_square.sh
├── train_1B_vec_rec.py
├── train_1B_vec_rec.sh
├── audio_extractor.py
├── vocal_seperator.py
├── requirement.txt
```

### 🧱 Audio Extraction

```bash
python audio_extractor.py --video_path="path/test/video.mp4" --saved_audio_path="path/test/audio.wav"
```

### 🧱 Vocal Separation

```bash
pip install audio-separator[gpu]
python vocal_seperator.py --audio_separator_model_file="path/StableAvatar/checkpoints/Kim_Vocal_2.onnx" --audio_file_path="path/test/audio.wav" --saved_vocal_path="path/test/vocal.wav"
```

### 🧱 Base Model Inference

```bash
bash inference.sh
```

*   Modify `--width` and `--height` in `inference.sh` for resolution.
*   `--output_dir`:  Output video save path.
*   `--validation_reference_path`, `--validation_driven_audio_path`, `--validation_prompts`: Paths for reference image, audio, and text prompts.
*   Prompts should follow the format: `[Description of first frame]-[Description of human behavior]-[Description of background (optional)]`.
*   `--pretrained_model_name_or_path`, `--pretrained_wav2vec_path`, `--transformer_path`: Pretrained model paths.
*   Adjust `--sample_steps`, `--overlap_window_length`, and `--clip_sample_n_frames` for quality and speed.
*   Recommended prompt and audio CFG is `[3-6]`.

Run a Gradio interface:

```bash
python app.py
```

### 💡Tips

*   Use either `transformer3d-square.pt` or `transformer3d-rec-vec.pt` based on dataset.
*   Adjust `--GPU_memory_mode` to `sequential_cpu_offload` or `model_cpu_offload` for reduced GPU memory usage.
*   Use `--ulysses_degree` and `--ring_degree` for multi-GPU inference.
*   Use ffmpeg to add audio to the output video.

### 🧱 Model Training

Follow the provided instructions for dataset organization and training scripts. The training setup consists of 8 nodes, each equipped with 8 NVIDIA A100 80GB GPUs, for training StableAvatar.

* Training StableAvatar requires approximately 50GB of VRAM due to the mixed-resolution (480x832 and 832x480) training pipeline.
* If you train StableAvatar exclusively on 512x512 videos, the VRAM requirement is reduced to approximately 40GB.
* The backgrounds of the selected training videos should remain static, as this helps the diffusion model calculate accurate reconstruction loss.
* The audio should be clear and free from excessive background noise.

### 🧱 Model Finetuning

Add `--transformer_path` to the finetuning scripts.

### 🧱 VRAM requirement and Runtime

For the 5s video (480x832, fps=25), the basic model (--GPU_memory_mode="model_full_load") requires approximately 18GB VRAM and finishes in 3 minutes on a 4090 GPU.

**🔥🔥Theoretically, StableAvatar is capable of synthesizing hours of video without significant quality degradation; however, the 3D VAE decoder demands significant GPU memory, especially when decoding 10k+ frames. You have the option to run the VAE decoder on CPU.🔥🔥**

## Contact

Email: francisshuyuan@gmail.com

**If you find our work useful, please consider giving a star ⭐ to this GitHub repository and citing it:**

```bib
@article{tu2025stableavatar,
  title={StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation},
  author={Tu, Shuyuan and Pan, Yueming and Huang, Yinming and Han, Xintong and Xing, Zhen and Dai, Qi and Luo, Chong and Wu, Zuxuan and Jiang Yu-Gang},
  journal={arXiv preprint arXiv:2508.08248},
  year={2025}
}
```