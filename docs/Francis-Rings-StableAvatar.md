# StableAvatar: Generate Infinite-Length Audio-Driven Avatar Videos

**StableAvatar enables the creation of high-fidelity, identity-preserving avatar videos of infinite length, driven by audio.** Explore the [original repository](https://github.com/Francis-Rings/StableAvatar) for more details.

<p align="center">
  <a href='https://francis-rings.github.io/StableAvatar'><img src='https://img.shields.io/badge/Project-Page-Green'></a> 
  <a href='https://arxiv.org/abs/2508.08248'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a> 
  <a href='https://huggingface.co/FrancisRing/StableAvatar/tree/main'><img src='https://img.shields.io/badge/HuggingFace-Model-orange'></a> 
  <a href='https://www.youtube.com/watch?v=6lhvmbzvv3Y'><img src='https://img.shields.io/badge/YouTube-Watch-red?style=flat-square&logo=youtube'></a> 
  <a href='https://www.bilibili.com/video/BV1hUt9z4EoQ'><img src='https://img.shields.io/badge/Bilibili-Watch-blue?style=flat-square&logo=bilibili'></a>
</p>

## Key Features

*   **Infinite-Length Video Generation:** Synthesize long-form avatar videos without compromising quality or identity.
*   **End-to-End Solution:**  Generates videos directly, *without* face-swapping or restoration post-processing, such as FaceFusion, GFP-GAN, and CodeFormer.
*   **Audio-Driven Animation:**  Creates realistic and synchronized avatar movements based on input audio.
*   **Identity Preservation:** Maintains the subject's identity throughout the entire video sequence.
*   **High-Fidelity Results:** Produces high-quality videos with natural audio synchronization.
*   **Flexible Resolution Support:** Compatible with multiple resolutions: 512x512, 480x832, and 832x480.

## Overview

StableAvatar is a cutting-edge video diffusion transformer designed to generate infinite-length, high-quality avatar videos driven by audio. The core innovation lies in its novel Time-step-aware Audio Adapter and Audio Native Guidance Mechanism, mitigating latent distribution errors and enhancing audio synchronization. A Dynamic Weighted Sliding-window Strategy ensures video smoothness.

<p align="center">
  <img src="assets/figures/framework.jpg" alt="model architecture" width="1280"/>
  <br/>
  <i>The overview of the framework of StableAvatar.</i>
</p>

## Demo Videos

<table border="0" style="width: 100%; text-align: left; margin-top: 20px;">
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/d7eca208-6a14-46af-b337-fb4d2b66ba8d" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/b5902ac4-8188-4da8-b9e6-6df280690ed1" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/87faa5c1-a118-4a03-a071-45f18e87e6a0" width="320" controls loop></video>
     </td>
  </tr>
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/531eb413-8993-4f8f-9804-e3c5ec5794d4" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/cdc603e2-df46-4cf8-a14e-1575053f996f" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/7022dc93-f705-46e5-b8fc-3a3fb755795c" width="320" controls loop></video>
     </td>
  </tr>
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/0ba059eb-ff6f-4d94-80e6-f758c613b737" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/03e6c1df-85c6-448d-b40d-aacb8add4e45" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/90b78154-dda0-4eaa-91fd-b5485b718a7f" width="320" controls loop></video>
     </td>
  </tr>
</table>

<p style="text-align: justify;">
  <span>Audio-driven avatar videos generated by StableAvatar.</span>
</p>

<p align="center">
  <video src="https://github.com/user-attachments/assets/90691318-311e-40b9-9bd9-62db83ab1492" width="768" autoplay loop muted playsinline></video>
  <br/>
  <span>Comparison results highlight the superior performance of StableAvatar.</span>
</p>


## Quickstart

### Setup
```bash
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu124
pip install -r requirements.txt
# Optional to install flash_attn to accelerate attention computation
pip install flash_attn
```

### Setup for Blackwell chips
```bash
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128
pip install -r requirements.txt
# Optional to install flash_attn to accelerate attention computation
pip install flash_attn
```

### Download Weights
```bash
pip install "huggingface_hub[cli]"
cd StableAvatar
mkdir checkpoints
huggingface-cli download FrancisRing/StableAvatar --local-dir ./checkpoints
```
### Quick Inference
Follow the steps below to generate an infinite-length avatar video.
1.  **Audio Extraction:** Extract audio from your video with `audio_extractor.py`:
    ```bash
    python audio_extractor.py --video_path="path/test/video.mp4" --saved_audio_path="path/test/audio.wav"
    ```
2.  **Vocal Separation (Optional):** Separate vocals from the audio to enhance lip-sync:
    ```bash
    pip install audio-separator[gpu]
    python vocal_seperator.py --audio_separator_model_file="path/StableAvatar/checkpoints/Kim_Vocal_2.onnx" --audio_file_path="path/test/audio.wav" --saved_vocal_path="path/test/vocal.wav"
    ```
3.  **Run Inference:** Use the `inference.sh` script.  Modify parameters such as resolution (`--width`, `--height`), output path (`--output_dir`), and audio/reference paths.  Prompts are key, use format: `[Description of first frame]-[Description of human behavior]-[Description of background (optional)]`.
    ```bash
    bash inference.sh
    ```
4.  **Gradio Interface:** Launch a Gradio interface using `app.py`.
    ```bash
    python app.py
    ```
5.  **Combine Audio and Video (Optional):** Use FFmpeg for MP4 with audio.
    ```bash
    ffmpeg -i video_without_audio.mp4 -i /path/audio.wav -c:v copy -c:a aac -shortest /path/output_with_audio.mp4
    ```

### Important Tips

*   Use prompts to guide the generation process.
*   Adjust parameters such as `--sample_steps`, `--overlap_window_length`, `--sample_text_guide_scale`, and `--sample_audio_guide_scale` to optimize the output.
*   GPU memory mode: To handle GPU memory limitations: Use `--GPU_memory_mode`.

### Training

StableAvatar can also be trained with custom datasets.  The required dataset structure and training scripts are documented in the original repository.

## To-Do List

-   \[x] StableAvatar-1.3B-basic
-   \[x] Inference Code
-   \[x] Data Pre-Processing Code (Audio Extraction)
-   \[x] Data Pre-Processing Code (Vocal Separation)
-   \[x] Training Code
-   \[x] Full Finetuning Code
-   \[x] Lora Training Code
-   \[x] Lora Finetuning Code
-   \[ ] Inference Code with Audio Native Guidance
-   \[ ] StableAvatar-pro

## Contact

For questions or suggestions, please reach out to:

Email: francisshuyuan@gmail.com

If you find our work useful, <b>please consider giving a star ⭐ to this github repository and citing it ❤️</b>:
```bib
@article{tu2025stableavatar,
  title={StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation},
  author={Tu, Shuyuan and Pan, Yueming and Huang, Yinming and Han, Xintong and Xing, Zhen and Dai, Qi and Luo, Chong and Wu, Zuxuan and Jiang Yu-Gang},
  journal={arXiv preprint arXiv:2508.08248},
  year={2025}
}