# StableAvatar: Generate Infinite-Length Avatar Videos from Audio

**Unleash the power of StableAvatar to create high-quality, infinitely long avatar videos driven by audio, without the need for post-processing. [Visit the Original Repo](https://github.com/Francis-Rings/StableAvatar)**

[![Project Page](https://img.shields.io/badge/Project-Page-Green)](https://francis-rings.github.io/StableAvatar)
[![Arxiv](https://img.shields.io/badge/Paper-Arxiv-red)](https://arxiv.org/abs/2508.08248)
[![HuggingFace Model](https://img.shields.io/badge/HuggingFace-Model-orange)](https://huggingface.co/FrancisRing/StableAvatar/tree/main)
[![YouTube](https://img.shields.io/badge/YouTube-Watch-red?style=flat-square&logo=youtube)](https://www.youtube.com/watch?v=6lhvmbzvv3Y)
[![Bilibili](https://img.shields.io/badge/Bilibili-Watch-blue?style=flat-square&logo=bilibili)](https://www.bilibili.com/video/BV1hUt9z4EoQ)

StableAvatar is a groundbreaking approach to audio-driven avatar video generation, enabling the creation of lifelike, infinite-length videos that preserve identity and synchronize perfectly with audio. This project offers a complete solution, from model implementation to training guides and comprehensive quickstart instructions.

## Key Features

*   **Infinite-Length Video Generation:** Generate avatar videos of any length without quality degradation.
*   **ID Preservation:** Maintains the subject's identity throughout the entire video.
*   **End-to-End Solution:** Synthesizes videos directly, eliminating the need for face-swapping or restoration tools.
*   **High-Fidelity Results:** Produce videos with exceptional quality and realistic audio synchronization.
*   **Time-step-aware Audio Adapter:** Designed to prevent error accumulation, crucial for long-form video generation.
*   **Audio Native Guidance Mechanism:** Enhances audio synchronization using the diffusion model's own predictions.
*   **Dynamic Weighted Sliding-window Strategy:** Ensures smooth transitions for continuous video generation.
*   **ComfyUI and Gradio Support:** Easy integration with ComfyUI and Gradio interfaces for user-friendly operation.

## Demo Videos

<table border="0" style="width: 100%; text-align: left; margin-top: 20px;">
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/d7eca208-6a14-46af-b337-fb4d2b66ba8d" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/b5902ac4-8188-4da8-b9e6-6df280690ed1" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/87faa5c1-a118-4a03-a071-45f18e87e6a0" width="320" controls loop></video>
     </td>
  </tr>
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/531eb413-8993-4f8f-9804-e3c5ec5794d4" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/cdc603e2-df46-4cf8-a14e-1575053f996f" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/7022dc93-f705-46e5-b8fc-3a3fb755795c" width="320" controls loop></video>
     </td>
  </tr>
  <tr>
      <td>
          <video src="https://github.com/user-attachments/assets/0ba059eb-ff6f-4d94-80e6-f758c613b737" width="320" controls loop></video>
      </td>
      <td>
          <video src="https://github.com/user-attachments/assets/03e6c1df-85c6-448d-b40d-aacb8add4e45" width="320" controls loop></video>
      </td>
       <td>
          <video src="https://github.com/user-attachments/assets/90b78154-dda0-4eaa-91fd-b5485b718a7f" width="320" controls loop></video>
     </td>
  </tr>
</table>

<p style="text-align: justify;">
  <span>Audio-driven avatar videos generated by StableAvatar, showcasing the ability to synthesize <b>infinite-length</b> and <b>ID-preserving videos</b>. All videos are <b>directly synthesized by StableAvatar without the use of any face-related post-processing tools</b>.</span>
</p>

<p align="center">
  <video src="https://github.com/user-attachments/assets/90691318-311e-40b9-9bd9-62db83ab1492" width="768" autoplay loop muted playsinline></video>
  <br/>
  <span>Comparison results between StableAvatar and state-of-the-art (SOTA) audio-driven avatar video generation models highlight the superior performance of StableAvatar.</span>
</p>

## Overview

<p align="center">
  <img src="assets/figures/framework.jpg" alt="model architecture" width="1280"/>
  </br>
  <i>The overview of the framework of StableAvatar.</i>
</p>

StableAvatar addresses the challenges of long-form video generation by incorporating innovative techniques to ensure audio synchronization, identity consistency, and smooth transitions. It utilizes a novel Time-step-aware Audio Adapter to prevent error accumulation, an Audio Native Guidance Mechanism to enhance audio-visual alignment, and a Dynamic Weighted Sliding-window Strategy for seamless video continuity.

## News

*   `[2025-8-18]`:üî• StableAvatar runs on [ComfyUI](https://github.com/smthemex/ComfyUI_StableAvatar) for faster generation. Thanks @[smthemex](https://github.com/smthemex)!
*   `[2025-8-16]`:üî• Finetuning and Lora training/finetuning codes released!
*   `[2025-8-15]`:üî• StableAvatar runs on a [Gradio Interface](https://space.bilibili.com/893892). Thanks @[gluttony-10]!
*   `[2025-8-15]`:üî• StableAvatar on [ComfyUI](https://github.com/smthemex/ComfyUI_StableAvatar). Thanks @[smthemex]!
*   `[2025-8-13]`:üî• Optimized for new Blackwell series Nvidia chips.
*   `[2025-8-11]`:üî• Project page, code, technical report, and [basic model checkpoint](https://huggingface.co/FrancisRing/StableAvatar/tree/main) released.

## üõ†Ô∏è To-Do List

*   [x] StableAvatar-1.3B-basic
*   [x] Inference Code
*   [x] Data Pre-Processing Code (Audio Extraction)
*   [x] Data Pre-Processing Code (Vocal Separation)
*   [x] Training Code
*   [x] Full Finetuning Code
*   [x] Lora Training Code
*   [x] Lora Finetuning Code
*   [ ] Inference Code with Audio Native Guidance
*   [ ] StableAvatar-pro

## üîë Quickstart

This quickstart guide provides instructions for getting started with the basic version of StableAvatar.

### üß± Environment Setup

```bash
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.1.1 --index-url https://download.pytorch.org/whl/cu124
pip install -r requirements.txt
# Optional to install flash_attn to accelerate attention computation
pip install flash_attn
```

### üß± Environment Setup for Blackwell series chips

```bash
pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128
pip install -r requirements.txt
# Optional to install flash_attn to accelerate attention computation
pip install flash_attn
```

### üß± Download Weights

```bash
pip install "huggingface_hub[cli]"
cd StableAvatar
mkdir checkpoints
huggingface-cli download FrancisRing/StableAvatar --local-dir ./checkpoints
```

### üß± Audio Extraction

```bash
python audio_extractor.py --video_path="path/test/video.mp4" --saved_audio_path="path/test/audio.wav"
```

### üß± Vocal Separation (Optional)

```bash
pip install audio-separator[gpu]
python vocal_seperator.py --audio_separator_model_file="path/StableAvatar/checkpoints/Kim_Vocal_2.onnx" --audio_file_path="path/test/audio.wav" --saved_vocal_path="path/test/vocal.wav"
```

### üß± Base Model Inference

```bash
bash inference.sh
```

For detailed configuration options, refer to the `inference.sh` file and the documentation.

### üß± Gradio Interface

```bash
python app.py
```

## üí° Tips

*   Wan2.1-1.3B-based StableAvatar weights have two versions: `transformer3d-square.pt` and `transformer3d-rec-vec.pt`
*   GPU memory usage can be optimized using the `--GPU_memory_mode` flag in `inference.sh`.
*   Multi-GPU inference is supported through `--ulysses_degree`, `--ring_degree`, and `--fsdp_dit` flags in `inference.sh`.
*   Use ffmpeg to add audio to your generated videos.

## üß± Model Training

Comprehensive instructions and dataset organization details are provided for training your own StableAvatar models. This section includes example dataset structures, training scripts, and hyperparameter explanations.

## üß± Model Finetuning

Instructions for finetuning StableAvatar models are provided, enabling you to adapt the model to your specific needs.

## üß± VRAM requirement and Runtime

For the 5s video (480x832, fps=25), the basic model (--GPU_memory_mode="model_full_load") requires approximately 18GB VRAM and finishes in 3 minutes on a 4090 GPU.

<b>üî•üî•Theoretically, StableAvatar is capable of synthesizing hours of video without significant quality degradation; however, the 3D VAE decoder demands significant GPU memory, especially when decoding 10k+ frames. You have the option to run the VAE decoder on CPU.üî•üî•</b>

## Contact

For suggestions or assistance, please contact:

Email: francisshuyuan@gmail.com

## Citation

If you find our work useful, please consider citing it ‚ù§Ô∏è:

```bib
@article{tu2025stableavatar,
  title={StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation},
  author={Tu, Shuyuan and Pan, Yueming and Huang, Yinming and Han, Xintong and Xing, Zhen and Dai, Qi and Luo, Chong and Wu, Zuxuan and Jiang Yu-Gang},
  journal={arXiv preprint arXiv:2508.08248},
  year={2025}
}
```