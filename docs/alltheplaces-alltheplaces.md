# All the Places: Your Source for Open Point of Interest (POI) Data

**All the Places** is a powerful web scraping project built with Scrapy, designed to gather and standardize Point of Interest (POI) data from various websites.  Visit the original repository on GitHub: [https://github.com/alltheplaces/alltheplaces](https://github.com/alltheplaces/alltheplaces)

## Key Features

*   **Data Sourcing:** Scrapes websites with store location pages to extract POI data.
*   **Standardized Format:** Publishes results in a consistent, easy-to-use format.
*   **Open Source:**  Leverages the robust Scrapy framework (Python-based) for web scraping.
*   **Community Driven:** Encourages contributions through comprehensive guides and open communication.
*   **Weekly Runs:**  Provides regularly updated data published on [alltheplaces.xyz](https://www.alltheplaces.xyz/).
*   **Open Data:** Data generated by the spiders is released under the [Creative Commons’ CC-0 waiver](https://creativecommons.org/publicdomain/zero/1.0/).

## Getting Started

### Development Setup

Follow the instructions below to set up a development environment for contributing to the project.  Windows users please consult the [Scrapy documentation](https://docs.scrapy.org/en/latest/intro/install.html#windows) for specific setup requirements.

#### Ubuntu

1.  **Install `uv`:**

    ```bash
    curl -LsSf https://astral.sh/uv/install.sh | sh
    source $HOME/.local/bin/env
    ```

2.  **Clone the Repository:**

    ```bash
    git clone git@github.com:alltheplaces/alltheplaces.git
    ```

3.  **Install Dependencies:**

    ```bash
    cd alltheplaces
    uv sync
    ```

4.  **Test Installation:**

    ```bash
    uv run scrapy
    ```

    If the above command runs without errors, your installation is successful.

#### macOS

1.  **Install `uv`:**

    ```bash
    brew install uv
    ```

2.  **Clone the Repository:**

    ```bash
    git clone git@github.com/alltheplaces/alltheplaces.git
    ```

3.  **Install Dependencies:**

    ```bash
    cd alltheplaces
    uv sync
    ```

4.  **Test Installation:**

    ```bash
    uv run scrapy
    ```

    If the above command runs without errors, your installation is successful.

#### Codespaces

You can use GitHub Codespaces for cloud-based development:

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/alltheplaces/alltheplaces)

#### Docker

1.  **Clone the Repository:**

    ```bash
    git clone git@github.com:alltheplaces/alltheplaces.git
    ```

2.  **Build the Docker Image:**

    ```bash
    cd alltheplaces
    docker build -t alltheplaces .
    ```

3.  **Run the Docker Container:**

    ```bash
    docker run --rm -it alltheplaces
    ```

### Contributing Code

We welcome contributions!  Here are some resources to help you get started:

*   [Spider Naming](docs/SPIDER_NAMING.md)
*   [Using Wikidata and the Name Suggestion Index](docs/WIKIDATA.md)
*   [Sitemaps and POI pages](docs/SITEMAP.md)
*   [Data from many POI pages can be extracted without writing code](docs/STRUCTURED_DATA.md)
*   [Pull Request Guidelines](docs/PULL_REQUEST.md)
*   [Behind the Scenes](docs/PIPELINES.md)

## The Weekly Run

The project's output is published weekly on [alltheplaces.xyz](https://www.alltheplaces.xyz/).  Please be mindful of website crawling etiquette.

## Contact Us

*   **GitHub Issues:** [Issue Tracker](https://github.com/alltheplaces/alltheplaces/issues)
*   **OSM US Slack:**  [#alltheplaces channel](https://osmus.slack.com/archives/C07EY4Y3M6F)

## License

*   **Data:** Released under [Creative Commons’ CC-0 waiver](https://creativecommons.org/publicdomain/zero/1.0/).
*   **Spider Software:** Licensed under the [MIT license](https://github.com/alltheplaces/alltheplaces/blob/master/LICENSE).