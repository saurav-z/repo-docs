# All the Places: Your Open-Source Solution for POI Data Extraction

**All the Places** is a powerful project that leverages web scraping to extract valuable Point of Interest (POI) data from websites, providing a structured and accessible dataset.  

[View the source code on GitHub](https://github.com/alltheplaces/alltheplaces)

## Key Features

*   **POI Data Extraction:** Scrapes websites to gather Point of Interest (POI) data, such as store locations.
*   **Scrapy Framework:** Utilizes the robust Scrapy framework for efficient and scalable web scraping.
*   **Standardized Data Format:** Outputs data in a consistent format for easy integration and use.
*   **Open Source:** Contribute to and customize the project to meet your specific needs.
*   **Regular Updates:** Data is regularly updated and published.
*   **Community Driven:** Connect and collaborate with other contributors through GitHub and other communication channels.

## Getting Started

### Development Setup

Follow these steps to set up your development environment.

#### Prerequisites

*   Python 3.x
*   Git

#### Installation using `uv` (Recommended)

1.  Install `uv`:

    *   **Ubuntu:**
        ```bash
        curl -LsSf https://astral.sh/uv/install.sh | sh
        source $HOME/.local/bin/env
        ```
    *   **macOS (using Homebrew):**
        ```bash
        brew install uv
        ```

2.  Clone the repository:
    ```bash
    git clone git@github.com:alltheplaces/alltheplaces.git
    ```

3.  Navigate to the project directory:
    ```bash
    cd alltheplaces
    ```

4.  Install dependencies:
    ```bash
    uv sync
    ```

5.  Verify installation:
    ```bash
    uv run scrapy
    ```
    If the command runs without errors, your installation is successful.

#### Codespaces

You can use GitHub Codespaces for a cloud-based development environment:

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/alltheplaces/alltheplaces)

#### Docker

You can also run the project using Docker:

1.  Clone the repository:
    ```bash
    git clone git@github.com:alltheplaces/alltheplaces.git
    ```

2.  Build the Docker image:
    ```bash
    cd alltheplaces
    docker build -t alltheplaces .
    ```

3.  Run the Docker container:
    ```bash
    docker run --rm -it alltheplaces
    ```

## Contributing

We welcome contributions!

### Guide to Contributing
*   [What should I call my spider?](docs/SPIDER_NAMING.md)
*   [Using Wikidata and the Name Suggestion Index](docs/WIKIDATA.md)
*   [Sitemaps make finding POI pages easier](docs/SITEMAP.md)
*   [Data from many POI pages can be extracted without writing code](docs/STRUCTURED_DATA.md)
*   [What is expected in a pull request?](docs/PULL_REQUEST.md)
*   [What we do behind the scenes](docs/PIPELINES.md)

## Weekly Run

The output of the project is published regularly on the website [alltheplaces.xyz](https://www.alltheplaces.xyz/). To avoid overwhelming websites, only run the necessary spiders.

## Contact

*   **Issues:** Report issues or ask questions via the GitHub [issue tracker](https://github.com/alltheplaces/alltheplaces/issues).
*   **Community:** Many contributors are available on the [OSM US Slack](https://slack.openstreetmap.us/) in the [#alltheplaces](https://osmus.slack.com/archives/C07EY4Y3M6F) channel.

## License

*   **Data:** The data generated by this project is available under the [Creative Commonsâ€™ CC-0 waiver](https://creativecommons.org/publicdomain/zero/1.0/).
*   **Software:** The spider software (this repository) is licensed under the [MIT license](https://github.com/alltheplaces/alltheplaces/blob/master/LICENSE).