# Llama Stack: Build Powerful AI Applications with Ease

**Llama Stack** simplifies AI application development by providing a unified API layer and a plugin architecture for seamless integration across various environments. Access the [original repository](https://github.com/meta-llama/llama-stack).

**Key Features:**

*   **Unified API Layer:**  Offers a standardized interface for Inference, RAG, Agents, Tools, Safety, Evals, and Telemetry.
*   **Plugin Architecture:** Supports a rich ecosystem of API implementations in diverse environments (local, cloud, on-premise, mobile).
*   **Prepackaged Distributions:** Provides ready-to-use solutions for quick and reliable application deployment.
*   **Multiple Developer Interfaces:** Includes CLI and SDKs for Python, Typescript, iOS, and Android for versatile development.
*   **Standalone Applications:** Offers examples to help you build production-grade AI applications.
*   **Llama 4 Support**: Supports the full suite of Meta's Llama 4 models.

**Get Started Quickly:**

```bash
curl -LsSf https://github.com/meta-llama/llama-stack/raw/main/install.sh | bash
```

**Why Choose Llama Stack?**

*   **Flexibility:** Choose your infrastructure without changing APIs.
*   **Consistency:** Build, test, and deploy with consistent application behavior.
*   **Ecosystem:** Leverage integrations with cloud providers, hardware vendors, and AI-focused companies.

**Llama Stack Benefits:**

*   Flexible Options: Developers can choose their preferred infrastructure without changing APIs and enjoy flexible deployment choices.
*   Consistent Experience: With its unified APIs, Llama Stack makes it easier to build, test, and deploy AI applications with consistent application behavior.
*   Robust Ecosystem: Llama Stack is already integrated with distribution partners (cloud providers, hardware vendors, and AI-focused companies) that offer tailored infrastructure, software, and services for deploying Llama models.

**API Providers:**

Llama Stack supports a wide range of API providers.  See the complete list in the [documentation](https://llama-stack.readthedocs.io/en/latest/providers/index.html).

**Distributions:**

Llama Stack Distributions streamline deployment.  Examples include:

*   Starter Distribution
*   Meta Reference
*   PostgreSQL

**Documentation:**

*   [Quick Start Guide](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html)
*   [CLI References](https://llama-stack.readthedocs.io/en/latest/references/llama_cli_reference/index.html)
*   [SDK Guides](https://llama-stack.readthedocs.io/en/latest/contributing/new_api_provider.html)
*   [Zero-to-Hero Guide](https://github.com/meta-llama/llama-stack/tree/main/docs/zero_to_hero_guide)
*   [Contributing Guide](CONTRIBUTING.md)

**Client SDKs:**

*   Python:  [llama-stack-client-python](https://github.com/meta-llama/llama-stack-client-python)
*   Swift:  [llama-stack-client-swift](https://github.com/meta-llama/llama-stack-client-swift)
*   Typescript: [llama-stack-client-typescript](https://github.com/meta-llama/llama-stack-client-typescript)
*   Kotlin: [llama-stack-client-kotlin](https://github.com/meta-llama/llama-stack-client-kotlin)

Explore the [llama-stack-apps](https://github.com/meta-llama/llama-stack-apps/tree/main/examples) repo for more examples.