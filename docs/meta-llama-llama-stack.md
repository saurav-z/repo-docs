# Llama Stack: Build Production-Ready AI Applications with Ease

Llama Stack simplifies AI application development by providing a unified API layer and a flexible plugin architecture, enabling developers to build and deploy cutting-edge generative AI applications.  Check out the original repo [here](https://github.com/meta-llama/llama-stack).

[![PyPI version](https://img.shields.io/pypi/v/llama_stack.svg)](https://pypi.org/project/llama_stack/)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-stack)](https://pypi.org/project/llama-stack/)
[![License](https://img.shields.io/pypi/l/llama_stack.svg)](https://github.com/meta-llama/llama-stack/blob/main/LICENSE)
[![Discord](https://img.shields.io/discord/1257833999603335178?color=6A7EC2&logo=discord&logoColor=ffffff)](https://discord.gg/llama-stack)
[![Unit Tests](https://github.com/meta-llama/llama-stack/actions/workflows/unit-tests.yml/badge.svg?branch=main)](https://github.com/meta-llama/llama-stack/actions/workflows/unit-tests.yml?query=branch%3Amain)
[![Integration Tests](https://github.com/meta-llama/llama-stack/actions/workflows/integration-tests.yml/badge.svg?branch=main)](https://github.com/meta-llama/llama-stack/actions/workflows/integration-tests.yml?query=branch%3Amain)
![coverage badge](./coverage.svg)

[**Quick Start**](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html) | [**Documentation**](https://llama-stack.readthedocs.io/en/latest/index.html) | [**Colab Notebook**](./docs/getting_started.ipynb) | [**Discord**](https://discord.gg/llama-stack)

## Key Features of Llama Stack:

*   **Unified API Layer:** Simplifies building AI applications with a consistent interface for inference, RAG, agents, tools, safety, evals, and telemetry.
*   **Plugin Architecture:** Supports a wide range of API implementations across various environments (local, cloud, on-premises, mobile).
*   **Prepackaged Distributions:** Offers ready-to-use solutions for quick and reliable deployment in any environment.
*   **Multiple Developer Interfaces:** Provides CLI and SDKs for Python, Typescript, iOS, and Android, catering to diverse development preferences.
*   **Standalone Applications:** Includes example applications demonstrating how to build production-grade AI applications with Llama Stack.

## âœ¨ðŸŽ‰ Llama 4 Support  ðŸŽ‰âœ¨

Version 0.2.0 introduced support for the Llama 4 models from Meta.

<details>

<summary>ðŸ‘‹ Click here to see how to run Llama 4 models on Llama Stack </summary>

\
*Note you need 8xH100 GPU-host to run these models*

```bash
pip install -U llama_stack

MODEL="Llama-4-Scout-17B-16E-Instruct"
# get meta url from llama.com
llama model download --source meta --model-id $MODEL --meta-url <META_URL>

# start a llama stack server
INFERENCE_MODEL=meta-llama/$MODEL llama stack build --run --template meta-reference-gpu

# install client to interact with the server
pip install llama-stack-client
```
### CLI
```bash
# Run a chat completion
MODEL="Llama-4-Scout-17B-16E-Instruct"

llama-stack-client --endpoint http://localhost:8321 \
inference chat-completion \
--model-id meta-llama/$MODEL \
--message "write a haiku for meta's llama 4 models"

ChatCompletionResponse(
    completion_message=CompletionMessage(content="Whispers in code born\nLlama's gentle, wise heartbeat\nFuture's soft unfold", role='assistant', stop_reason='end_of_turn', tool_calls=[]),
    logprobs=None,
    metrics=[Metric(metric='prompt_tokens', value=21.0, unit=None), Metric(metric='completion_tokens', value=28.0, unit=None), Metric(metric='total_tokens', value=49.0, unit=None)]
)
```
### Python SDK
```python
from llama_stack_client import LlamaStackClient

client = LlamaStackClient(base_url=f"http://localhost:8321")

model_id = "meta-llama/Llama-4-Scout-17B-16E-Instruct"
prompt = "Write a haiku about coding"

print(f"User> {prompt}")
response = client.inference.chat_completion(
    model_id=model_id,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt},
    ],
)
print(f"Assistant> {response.completion_message.content}")
```
As more providers start supporting Llama 4, you can use them in Llama Stack as well. We are adding to the list. Stay tuned!


</details>

## ðŸš€ One-Line Installation

Get started with Llama Stack locally:

```bash
curl -LsSf https://github.com/meta-llama/llama-stack/raw/main/scripts/install.sh | bash
```

## Overview

Llama Stack streamlines the development of AI applications by providing standardized building blocks and best practices. It offers a unified API layer and a plugin architecture to support a wide variety of API implementations in different environments. Prepackaged distributions are available to get developers started quickly and reliably. Multiple developer interfaces are supported including CLI and SDKs for Python, Typescript, iOS, and Android. Standalone applications are included as examples for building production-grade AI applications.

<div style="text-align: center;">
  <img
    src="https://github.com/user-attachments/assets/33d9576d-95ea-468d-95e2-8fa233205a50"
    width="480"
    title="Llama Stack"
    alt="Llama Stack"
  />
</div>

## Benefits of Llama Stack:

*   **Flexible Deployment:** Choose your preferred infrastructure without changing APIs, with flexible deployment options.
*   **Consistent Experience:** Build, test, and deploy AI applications with consistent application behavior through unified APIs.
*   **Robust Ecosystem:** Leverage integrations with distribution partners for tailored infrastructure, software, and services for Llama model deployment.

## API Providers

Llama Stack integrates with various API providers.  See the [full list](https://llama-stack.readthedocs.io/en/latest/providers/index.html) for details.

| API Provider Builder | Environments | Agents | Inference | VectorIO | Safety | Telemetry | Post Training | Eval | DatasetIO |
|:-------------------:|:------------:|:------:|:---------:|:--------:|:------:|:---------:|:-------------:|:----:|:--------:|
| Meta Reference | Single Node | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… | âœ… |
| SambaNova | Hosted | | âœ… | | âœ… | | | | |
| Cerebras | Hosted | | âœ… | | | | | | |
| Fireworks | Hosted | âœ… | âœ… | âœ… | | | | | |
| AWS Bedrock | Hosted | | âœ… | | âœ… | | | | |
| Together | Hosted | âœ… | âœ… | | âœ… | | | | |
| Groq | Hosted | | âœ… | | | | | | |
| Ollama | Single Node | | âœ… | | | | | | |
| TGI | Hosted/Single Node | | âœ… | | | | | | |
| NVIDIA NIM | Hosted/Single Node | | âœ… | | âœ… | | | | |
| ChromaDB | Hosted/Single Node | | | âœ… | | | | | |
| PG Vector | Single Node | | | âœ… | | | | | |
| PyTorch ExecuTorch | On-device iOS | âœ… | âœ… | | | | | | |
| vLLM | Single Node | | âœ… | | | | | | |
| OpenAI | Hosted | | âœ… | | | | | | |
| Anthropic | Hosted | | âœ… | | | | | | |
| Gemini | Hosted | | âœ… | | | | | | |
| WatsonX | Hosted | | âœ… | | | | | | |
| HuggingFace | Single Node | | | | | | âœ… | | âœ… |
| TorchTune | Single Node | | | | | | âœ… | | |
| NVIDIA NEMO | Hosted | | âœ… | âœ… | | | âœ… | âœ… | âœ… |
| NVIDIA | Hosted | | | | | | âœ… | âœ… | âœ… |

> **Note**: Additional providers are available through external packages. See [External Providers](https://llama-stack.readthedocs.io/en/latest/providers/external.html) documentation.

## Distributions

Llama Stack Distributions offer pre-configured bundles of provider implementations for each API component, simplifying deployment.

|               **Distribution**                |                                                                    **Llama Stack Docker**                                                                     |                                                 Start This Distribution                                                  |
|:---------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------------:|
|                Starter Distribution                 |           [llamastack/distribution-starter](https://hub.docker.com/repository/docker/llamastack/distribution-starter/general)           |      [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/starter.html)      |
|                Meta Reference                 |           [llamastack/distribution-meta-reference-gpu](https://hub.docker.com/repository/docker/llamastack/distribution-meta-reference-gpu/general)           |      [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/meta-reference-gpu.html)      |
|                   PostgreSQL                  |                [llamastack/distribution-postgres-demo](https://hub.docker.com/repository/docker/llamastack/distribution-postgres-demo/general)                |                  |

## Documentation

Explore the following documentation resources:

*   [CLI References](https://llama-stack.readthedocs.io/en/latest/references/llama_cli_reference/index.html): Detailed guides for the `llama` and `llama-stack-client` CLIs.
*   [Getting Started](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html): Quick guides to setting up and using Llama Stack.
*   [Jupyter Notebook](./docs/getting_started.ipynb): Interactive tutorials with example code.
*   [Colab Notebook](https://colab.research.google.com/drive/1dtVmxotBsI4cGZQNsJRYPrLiDeT0Wnwt): Complete Llama Stack lesson from the Deeplearning.ai course.
*   [Zero-to-Hero Guide](https://github.com/meta-llama/llama-stack/tree/main/docs/zero_to_hero_guide): Comprehensive guide with code samples.
*   [Contributing](CONTRIBUTING.md): Learn how to contribute to Llama Stack.
*   [Adding a new API Provider](https://llama-stack.readthedocs.io/en/latest/contributing/new_api_provider.html): Steps for adding a new API provider.

## Llama Stack Client SDKs

Choose your preferred SDK for connecting to a Llama Stack server:

|  **Language** |  **Client SDK** | **Package** |
| :----: | :----: | :----: |
| Python |  [llama-stack-client-python](https://github.com/meta-llama/llama-stack-client-python) | [![PyPI version](https://img.shields.io/pypi/v/llama_stack_client.svg)](https://pypi.org/project/llama_stack_client/)
| Swift  | [llama-stack-client-swift](https://github.com/meta-llama/llama-stack-client-swift) | [![Swift Package Index](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fmeta-llama%2Fllama-stack-client-swift%2Fbadge%3Ftype%3Dswift-versions)](https://swiftpackageindex.com/meta-llama/llama-stack-client-swift)
| Typescript   | [llama-stack-client-typescript](https://github.com/meta-llama/llama-stack-client-typescript) | [![NPM version](https://img.shields.io/npm/v/llama-stack-client.svg)](https://npmjs.org/package/llama-stack-client)
| Kotlin | [llama-stack-client-kotlin](https://github.com/meta-llama/llama-stack-client-kotlin) | [![Maven version](https://img.shields.io/maven-central/v/com.llama.llamastack/llama-stack-client-kotlin)](https://central.sonatype.com/artifact/com.llama.llamastack/llama-stack-client-kotlin)

Find more example scripts with client SDKs in the [llama-stack-apps](https://github.com/meta-llama/llama-stack-apps/tree/main/examples) repo.