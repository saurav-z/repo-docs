# ðŸ§ ðŸ¤– Deep Agents: Build Powerful AI Agents with Ease

**Unlock the potential of sophisticated AI agents capable of complex tasks and long-term planning.** Deep Agents empowers you to create advanced agents by leveraging key components like planning, sub-agents, and file system access.

[View the original repository](https://github.com/hwchase17/deepagents)

**Key Features:**

*   **Simplified Agent Creation:** Easily build deep agents by combining tools, instructions, and optional subagents.
*   **Built-in Components:** Benefit from a pre-configured system prompt, planning tool, virtual file system, and sub-agent capabilities.
*   **Flexible Customization:** Tailor agents to your needs with customizable tools, instructions, models, and sub-agent configurations.
*   **Human-in-the-Loop Support:** Integrate human approval for tool execution for enhanced control and safety.
*   **LangGraph Compatibility:** Deep Agents seamlessly integrates with LangGraph, enabling advanced features such as streaming, memory, and studio integration.
*   **MCP Compatibility:** Deep Agents supports [Langchain MCP Adapter library](https://github.com/langchain-ai/langchain-mcp-adapters).

## Installation

```bash
pip install deepagents
```

## Usage Examples

### Basic Research Agent

(Requires `pip install tavily-python`)

```python
import os
from typing import Literal

from tavily import TavilyClient
from deepagents import create_deep_agent

tavily_client = TavilyClient(api_key=os.environ["TAVILY_API_KEY"])

# Search tool
def internet_search(
    query: str,
    max_results: int = 5,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = False,
):
    """Run a web search"""
    return tavily_client.search(
        query,
        max_results=max_results,
        include_raw_content=include_raw_content,
        topic=topic,
    )


# Instructions
research_instructions = """You are an expert researcher. Your job is to conduct thorough research, and then write a polished report.

You have access to a few tools.

## `internet_search`

Use this to run an internet search for a given query. You can specify the number of results, the topic, and whether raw content should be included.
"""

# Create the agent
agent = create_deep_agent(
    [internet_search],
    research_instructions,
)

# Invoke the agent
result = agent.invoke({"messages": [{"role": "user", "content": "what is langgraph?"}]})
```

See [examples/research/research_agent.py](examples/research/research_agent.py) for a more complex example.

## Creating a Custom Deep Agent

The `create_deep_agent` function is the core of this library. Here's how to customize it.

### Tools (Required)

Pass a list of functions or LangChain `@tool` objects that the agent will use.

### Instructions (Required)

Provide instructions that will be incorporated into the agent's prompt.
This is combined with a [built-in system prompt](src/deepagents/prompts.py).

### Subagents (Optional)

Define subagents with custom prompts and tool access for specialized tasks.

```python
# Example subagent definition
research_subagent = {
    "name": "research-agent",
    "description": "Used to research more in depth questions",
    "prompt": sub_research_prompt,
}
subagents = [research_subagent]
agent = create_deep_agent(
    tools,
    prompt,
    subagents=subagents
)
```

### Model (Optional)

Customize the LLM used by the agent. Defaults to `"claude-sonnet-4-20250514"`.

```python
from deepagents import create_deep_agent

model = init_chat_model(
    model="ollama:gpt-oss:20b",  
)
agent = create_deep_agent(
    tools=tools,
    instructions=instructions,
    model=model,
    ...
)
```

### Built-in Tools (Optional)

Control which built-in tools are available to the agent.

```python
# Only give agent access to todo tool, none of the filesystem tools
builtin_tools = ["write_todos"]
agent = create_deep_agent(..., builtin_tools=builtin_tools, ...)
```

### Per-subagent model override (optional)

Use a fast, deterministic model for a critique sub-agent, while keeping a different default model for the main agent and others:

```python
from deepagents import create_deep_agent

critique_sub_agent = {
    "name": "critique-agent",
    "description": "Critique the final report",
    "prompt": "You are a tough editor.",
    "model_settings": {
        "model": "anthropic:claude-3-5-haiku-20241022",
        "temperature": 0,
        "max_tokens": 8192
    }
}

agent = create_deep_agent(
    tools=[internet_search],
    instructions="You are an expert researcher...",
    model="claude-sonnet-4-20250514",  # default for main agent and other sub-agents
    subagents=[critique_sub_agent],
)
```

## Deep Agent Components

### System Prompt

A detailed, customizable system prompt is included to guide agent behavior, inspired by Claude Code.

### Planning Tool

A built-in tool to enable the agent to create a plan, and stay on track, inspired by Claude Code.

### File System Tools

Basic file system tools (`ls`, `edit_file`, `read_file`, `write_file`) are provided using LangGraph's State object.

### Sub Agents

Deep Agents support the use of subagents for specialized tasks.

### Built In Tools

Five built-in tools: `write_todos`, `write_file`, `read_file`, `ls`, `edit_file`. These can be disabled via the [`builtin_tools`](#builtintools--optional-) parameter.

### Human-in-the-Loop

Integrate human approval for tool execution using `interrupt_config`, supporting "approve", "edit", and "respond" actions.

```python
from deepagents import create_deep_agent
from langgraph.checkpoint.memory import InMemorySaver

# Create agent with file operations requiring approval
agent = create_deep_agent(
    tools=[your_tools],
    instructions="Your instructions here",
    interrupt_config={
        # You can specify a dictionary for fine grained control over what interrupt options exist
        "tool_1": {
            "allow_ignore": False,
            "allow_respond": True,
            "allow_edit": True,
            "allow_accept":True,
        },
        # You can specify a boolean for shortcut
        # This is a shortcut for the same functionality as above
        "tool_2": True,
    }
)

checkpointer= InMemorySaver()
agent.checkpointer = checkpointer
```

#### Approve

```python
config = {"configurable": {"thread_id": "1"}}
for s in agent.stream({"messages": [{"role": "user", "content": message}]}, config=config):
    print(s)
# If this calls a tool with an interrupt, this will then return an interrupt
for s in agent.stream(Command(resume=[{"type": "accept"}]), config=config):
    print(s)

```

#### Edit

```python
config = {"configurable": {"thread_id": "1"}}
for s in agent.stream({"messages": [{"role": "user", "content": message}]}, config=config):
    print(s)
# If this calls a tool with an interrupt, this will then return an interrupt
# Replace the `...` with the tool name you want to call, and the arguments
for s in agent.stream(Command(resume=[{"type": "edit", "args": {"action": "...", "args": {...}}}]), config=config):
    print(s)

```

#### Respond

```python
config = {"configurable": {"thread_id": "1"}}
for s in agent.stream({"messages": [{"role": "user", "content": message}]}, config=config):
    print(s)
# If this calls a tool with an interrupt, this will then return an interrupt
# Replace the `...` with the response to use all the ToolMessage content
for s in agent.stream(Command(resume=[{"type": "response", "args": "..."}]), config=config):
    print(s)

```

## Langchain MCP Adapter

The `deepagents` library can be ran with MCP tools. This can be achieved by using the [Langchain MCP Adapter library](https://github.com/langchain-ai/langchain-mcp-adapters).

(To run the example below, will need to `pip install langchain-mcp-adapters`)

```python
import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient
from deepagents import create_deep_agent

async def main():
    # Collect MCP tools
    mcp_client = MultiServerMCPClient(...)
    mcp_tools = await mcp_client.get_tools()

    # Create agent
    agent = create_deep_agent(tools=mcp_tools, ....)

    # Stream the agent
    async for chunk in agent.astream(
        {"messages": [{"role": "user", "content": "what is langgraph?"}]},
        stream_mode="values"
    ):
        if "messages" in chunk:
            chunk["messages"][-1].pretty_print()

asyncio.run(main())
```

## Roadmap

*   \[ ] Allow users to customize full system prompt
*   \[ ] Code cleanliness (type hinting, docstrings, formatting)
*   \[ ] Allow for more of a robust virtual filesystem
*   \[ ] Create an example of a deep coding agent built on top of this
*   \[ ] Benchmark the example of [deep research agent](examples/research/research_agent.py)